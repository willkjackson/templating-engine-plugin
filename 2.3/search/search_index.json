{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jenkins Templating Engine \u00b6 Welcome! The Jenkins Templating Engine is a pipeline development framework for Jenkins that allows teams to create tool-agnostic Pipeline Templates that can be reused across applications. Concepts Concept pages are understanding oriented articles describing how JTE works. Reference Reference pages are information oriented descriptions of JTE mechanics. Tutorials Tutorials are learning oriented lessons to teach users about JTE. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems. Work In Progress This docs site is still a work in progress! Check out the contributing section if you're interested in helping.","title":"Home"},{"location":"#jenkins-templating-engine","text":"Welcome! The Jenkins Templating Engine is a pipeline development framework for Jenkins that allows teams to create tool-agnostic Pipeline Templates that can be reused across applications. Concepts Concept pages are understanding oriented articles describing how JTE works. Reference Reference pages are information oriented descriptions of JTE mechanics. Tutorials Tutorials are learning oriented lessons to teach users about JTE. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems. Work In Progress This docs site is still a work in progress! Check out the contributing section if you're interested in helping.","title":"Jenkins Templating Engine"},{"location":"glossary/","text":"","title":"Glossary"},{"location":"concepts/advanced/overloading-steps/","text":"Overloading Steps \u00b6 Function Overloading 1 is when there are multiple methods with the same name and different implementation. Conflicting Library Steps \u00b6 To allow multiple libraries to contribute the same step, the Pipeline Configuration must have jte.permissive_initialization set to true. If multiple libraries do contribute the same step, the step will no longer be able to be invoked by its short_name. Instead, overloaded steps must be accessed using the Pipeline Primitive Namespace . Overloaded Library Steps The following example assumes a gradle and npm library are available that both contribute a build() step. Pipeline Configuration jte { permissive_initialization = true // pipeline will fail if not set } libraries { npm // contributes build() gradle // contributes build() } Pipeline Template // build() <-- would fail because step is overloaded jte . libraries . npm . build () jte . libraries . gradle . build () Default Mode is to Fail By default, if two libraries are loaded that contribute the same step then the Pipeline Run will fail during Pipeline Initialization . This behavior is modified by setting jte.permissive_initialization to True . Library Steps Overriding Jenkins Pipeline Steps \u00b6 Jenkins Pipeline DSL steps can be overridden by Library Steps . If a Library Step has the same name as a Jenkins Pipeline DSL step, such as node or build , the Library Step will take precedence. To invoke the original Jenkins Pipeline DSL Step, use the steps Global Variable . Declarative Syntax This isn't true for Declarative Syntax. Check out the Declarative Syntax page to learn more. Use Case: Overriding node If a library were to contribute a Library Step called node , then it would override the node step used in Jenkins Scripted Pipelines. The following example shows how to override the default node step to augment its functionality. node.groovy // support the original node interface void call ( String label = null , Closure body ){ if ( label ){ steps . node ( label , body ) // steps var used to access original \"node\" implementation } else { steps . node ( body ) } } // support new functionality void call ( Map args = [:], Closure body ){ if ( args . containsKey ( \"container\" )){ docker . image ( args . container ). inside ( body ) } } Pipeline Template // assume the Pipeline Configuration loaded the library contributing node.groovy node { println \"hi\" } node ( \"my-label\" ){ println \"hi\" } node ( container: \"maven\" ){ sh \"mvn -v\" } // custom functionality Function Overloading \u21a9","title":"Overloading Steps"},{"location":"concepts/advanced/overloading-steps/#overloading-steps","text":"Function Overloading 1 is when there are multiple methods with the same name and different implementation.","title":"Overloading Steps"},{"location":"concepts/advanced/overloading-steps/#conflicting-library-steps","text":"To allow multiple libraries to contribute the same step, the Pipeline Configuration must have jte.permissive_initialization set to true. If multiple libraries do contribute the same step, the step will no longer be able to be invoked by its short_name. Instead, overloaded steps must be accessed using the Pipeline Primitive Namespace . Overloaded Library Steps The following example assumes a gradle and npm library are available that both contribute a build() step. Pipeline Configuration jte { permissive_initialization = true // pipeline will fail if not set } libraries { npm // contributes build() gradle // contributes build() } Pipeline Template // build() <-- would fail because step is overloaded jte . libraries . npm . build () jte . libraries . gradle . build () Default Mode is to Fail By default, if two libraries are loaded that contribute the same step then the Pipeline Run will fail during Pipeline Initialization . This behavior is modified by setting jte.permissive_initialization to True .","title":"Conflicting Library Steps"},{"location":"concepts/advanced/overloading-steps/#library-steps-overriding-jenkins-pipeline-steps","text":"Jenkins Pipeline DSL steps can be overridden by Library Steps . If a Library Step has the same name as a Jenkins Pipeline DSL step, such as node or build , the Library Step will take precedence. To invoke the original Jenkins Pipeline DSL Step, use the steps Global Variable . Declarative Syntax This isn't true for Declarative Syntax. Check out the Declarative Syntax page to learn more. Use Case: Overriding node If a library were to contribute a Library Step called node , then it would override the node step used in Jenkins Scripted Pipelines. The following example shows how to override the default node step to augment its functionality. node.groovy // support the original node interface void call ( String label = null , Closure body ){ if ( label ){ steps . node ( label , body ) // steps var used to access original \"node\" implementation } else { steps . node ( body ) } } // support new functionality void call ( Map args = [:], Closure body ){ if ( args . containsKey ( \"container\" )){ docker . image ( args . container ). inside ( body ) } } Pipeline Template // assume the Pipeline Configuration loaded the library contributing node.groovy node { println \"hi\" } node ( \"my-label\" ){ println \"hi\" } node ( container: \"maven\" ){ sh \"mvn -v\" } // custom functionality Function Overloading \u21a9","title":"Library Steps Overriding Jenkins Pipeline Steps"},{"location":"concepts/advanced/pipeline-initialization/","text":"Pipeline Initialization \u00b6 Pipeline Initialization is the term JTE uses to describe everything that happens from when a Pipeline Run is started to when the Pipeline starts. Overview \u00b6 This stages of Pipeline Initialization are described in the table and picture below. Stage Description Pipeline Configuration Aggregation Pipeline Configurations fetched from Governance Tiers in the Configuration Hierarchy and merged, creating an Aggregated Pipeline Configuration Pipeline Primitives Injected JTE parses the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives Pipeline Template Determined JTE determines the Pipeline Template to use according to the process outlined in Pipeline Template Selection Pipeline Template Compiled The groovy Pipeline Template is compiled into the final form that will be executed the same way as any other Jenkinsfile Pipeline Configuration Aggregation \u00b6 During the Pipeline Configuration Aggregation phase, JTE fetches the Pipeline Configuration from each Governance Tier (if present) in the Configuration Hierarchy . These Pipeline Configurations are then sequentially merged, according to the procedure outlined in Merging Pipeline Configurations . The result of this process is an aggregated Pipeline Configuration for the Pipeline Run. The Map representation of the aggregated Pipeline Configuration is made available via the pipelineConfig Autowired Variable . Pipeline Primitives Injected \u00b6 This next phase of Pipeline Initialization takes the aggregated Pipeline Configuration and parses it to create Pipeline Primitives . This happens by passing the aggregated Pipeline Configuration to a series of Template Primitive Injectors. Each Pipeline Primitive has a corresponding Template Primitive Injector that reads the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives. The Pipeline Primitives created during this phase are stored on the Pipeline Primitive Namespace . Important You may remember from reading the Pipeline Configuration Syntax that JTE's Pipeline Configuration DSL is just a dynamic builder language, meaning that it will accept any configurations it is given. The only reason application_environments does anything is because there is a Template Primitive Injector that specifically looks for that block. Pipeline Template Determined \u00b6 Next, JTE determines the Pipeline Template based on the process outlined in Pipeline Template Selection . Pipeline Template Compiled \u00b6 Finally, the determined Pipeline Template is compiled. JTE performs some Compile-Time Metaprogramming to wrap the Pipeline Template in a Try-Catch-Finally block so that the Lifecycle Hooks can be executed before and after Pipeline Template Execution. This compiled Pipeline is then executed just like any other Jenkinsfile . Pipeline Template Compilation JTE performs compile-time metaprogramming to slightly manipulate the Pipeline Template. What follows is an example to understand the transformation that takes place. Provided Pipeline Template build () Compiled Pipeline Template try { // code that invokes @Validation annotated methods in steps // code that invokes @Init annotated methods in steps build () // <-- original Pipeline Template } finally { // code that invokes @CleanUp annotated methods in steps // code that invokes @Notify annotated methods in steps }","title":"Pipeline Initialization"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-initialization","text":"Pipeline Initialization is the term JTE uses to describe everything that happens from when a Pipeline Run is started to when the Pipeline starts.","title":"Pipeline Initialization"},{"location":"concepts/advanced/pipeline-initialization/#overview","text":"This stages of Pipeline Initialization are described in the table and picture below. Stage Description Pipeline Configuration Aggregation Pipeline Configurations fetched from Governance Tiers in the Configuration Hierarchy and merged, creating an Aggregated Pipeline Configuration Pipeline Primitives Injected JTE parses the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives Pipeline Template Determined JTE determines the Pipeline Template to use according to the process outlined in Pipeline Template Selection Pipeline Template Compiled The groovy Pipeline Template is compiled into the final form that will be executed the same way as any other Jenkinsfile","title":"Overview"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-configuration-aggregation","text":"During the Pipeline Configuration Aggregation phase, JTE fetches the Pipeline Configuration from each Governance Tier (if present) in the Configuration Hierarchy . These Pipeline Configurations are then sequentially merged, according to the procedure outlined in Merging Pipeline Configurations . The result of this process is an aggregated Pipeline Configuration for the Pipeline Run. The Map representation of the aggregated Pipeline Configuration is made available via the pipelineConfig Autowired Variable .","title":"Pipeline Configuration Aggregation"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-primitives-injected","text":"This next phase of Pipeline Initialization takes the aggregated Pipeline Configuration and parses it to create Pipeline Primitives . This happens by passing the aggregated Pipeline Configuration to a series of Template Primitive Injectors. Each Pipeline Primitive has a corresponding Template Primitive Injector that reads the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives. The Pipeline Primitives created during this phase are stored on the Pipeline Primitive Namespace . Important You may remember from reading the Pipeline Configuration Syntax that JTE's Pipeline Configuration DSL is just a dynamic builder language, meaning that it will accept any configurations it is given. The only reason application_environments does anything is because there is a Template Primitive Injector that specifically looks for that block.","title":"Pipeline Primitives Injected"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-template-determined","text":"Next, JTE determines the Pipeline Template based on the process outlined in Pipeline Template Selection .","title":"Pipeline Template Determined"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-template-compiled","text":"Finally, the determined Pipeline Template is compiled. JTE performs some Compile-Time Metaprogramming to wrap the Pipeline Template in a Try-Catch-Finally block so that the Lifecycle Hooks can be executed before and after Pipeline Template Execution. This compiled Pipeline is then executed just like any other Jenkinsfile . Pipeline Template Compilation JTE performs compile-time metaprogramming to slightly manipulate the Pipeline Template. What follows is an example to understand the transformation that takes place. Provided Pipeline Template build () Compiled Pipeline Template try { // code that invokes @Validation annotated methods in steps // code that invokes @Init annotated methods in steps build () // <-- original Pipeline Template } finally { // code that invokes @CleanUp annotated methods in steps // code that invokes @Notify annotated methods in steps }","title":"Pipeline Template Compiled"},{"location":"concepts/framework-overview/bottom-up/","text":"Bottom-Up Explanation \u00b6 Welcome ! This page is going to take you on a journey from how pipelines are typically built today (on a per-application basis), pausing to look at the challenges that causes at scale, and then step through together how the Jenkins Templating Engine (JTE) works to remediate those pain points. This will be one of the lengthier pages in the documentation. If you can stick to it, you'll come out the other side seeing pipeline development differently. Writing Pipelines Without the Jenkins Templating Engine \u00b6 Imagine that there are three applications that each need a pipeline to automate the execution of unit tests and building an artifact. Click through the tabs below to see a pipeline for a Gradle application, a Maven application, and an NPM application. Gradle // a basic Gradle pipeline stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } stage ( \"Build\" ){ node { sh \"gradle build\" } } Maven // a basic Maven pipeline stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } stage ( \"Build\" ){ node { sh \"mvn build\" } } NPM // a basic NPM pipeline stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } Traditionally, these pipelines would be stored alongside the source code for the application. Why This Approach Doesn't Scale \u00b6 Defining pipelines on a per-application basis works when you have a small number of teams you're supporting . Over time, though, \"DevOps Teams\" at organizations find themselves responsible for administering a growing number of internal tools and scaling the adoption of those tools across application teams. Why is creating bespoke pipelines such a big deal? It doesn't scale. Individual pipelines means work needs to be done to integrate each application with a pipeline Duplicated pipelines introduce uncertainty that common processes are followed Complexity becomes difficult to manage Onboarding Each Team Individually \u00b6 When pipelines are built on a per-application basis, it leaves organizations with two choices. Either you need a developer on every team who knows how to write pipelines aligned with organizational standards, or you need a dedicated pipeline team onboarding application teams to a pipeline. The first choice often isn't super viable - developers should focus on the problem that they're best suited to solve: developing applications. Even if all the software developers could write their own automated pipelines, it becomes very challenging to ensure these pipelines follow required compliance checks. The second choice requires that you scale the dedicated pipeline team to meet the number of application teams that need support. This is often prohibitively expensive. Standardization & Compliance \u00b6 When pipelines are built on a per-application basis, it becomes extremely challenging to be confident that teams throughout the organization are following approved software delivery processes aligned with cyber compliance requirements. Furthermore, for more tightly governed environments, the idea of a developer being able to modify the Jenkinsfile on their branch to perform a deployment to production is a threat vector that introduces unacceptable, unnecessary risks. Mitigating this risk requires very mature monitoring and correlation across systems. Complexity \u00b6 There's an old adage, \"running one container is easy, running many is really, really hard.\" The same applies for pipelines within an organization. Creating a pipeline that runs unit tests, builds artifacts, deploys those artifacts, and runs security scans gets exponentially more complex the more teams and technical stacks need to be supported. A Better Way: Pipeline Templating \u00b6 These challenges are all caused by building pipelines on a per-application basis . Even if you're modularizing your pipeline code for reuse through Jenkins Shared Libraries, you still end up duplicating the Jenkinsfile across every branch of every source code repository. Warning Don't even get me started on how tricky it can be to propagate a change to the pipeline across teams when the representation of the pipeline lives across each branch independently. Taking a step back, you may have noticed that each of the three pipelines above follow the same structure. First they execute unit tests, then they perform a build. In our experience, this simple pattern holds true most of the time ( especially when working with microservices). While the specific tools that get used to perform a step of the pipeline may change, the workflow remains the same. Writing a Pipeline Template \u00b6 The entire philosophy behind the Jenkins Template Engine stems from the concept of common workflows with interchangeable tools. What if it was possible to translate the three separate pipelines above into the following Pipeline Template: // file: Jenkinsfile unit_test () build () Good news, this is exactly what JTE makes possible! Next, you'll need to define your implementations of the unit_test() and build() steps. Writing Pipeline Steps \u00b6 Implement the unit_test() and build() steps by refactoring the original pipelines above. Gradle: unit_test // file: gradle/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } } Gradle: build // file: gradle/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"gradle build\" } } } Maven: unit_test // file: maven/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } } Maven: build // file: maven/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"mvn build\" } } } NPM: unit_test // file: npm/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } } NPM: build // file: npm/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } } Note These steps are the exact same pipeline code we had written at the start. We just wrapped it in a call method for Reasons you can learn about over on the library steps page. Understanding Libraries \u00b6 If you go back and look at the comments indicating the files those steps are placed in, you'll notice the following file structure: . \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u2514\u2500\u2500 npm \u2514\u2500\u2500 steps \u251c\u2500\u2500 build.groovy \u2514\u2500\u2500 unit_test.groovy A library in JTE is a collection of steps (stored together in a directory) that can be loaded at runtime. Earlier, a common Pipeline Template was defined that executes unit tests and then builds an artifact. This template can be shared across teams. Then, three libraries were created: npm , gradle , and maven . Each of these libraries implemented the steps required by the template. Finally, JTE needs a way to determine which libraries to load for a given team's pipeline. Pipeline Configuration \u00b6 So far, you've defined a Pipeline Template that invokes steps, and libraries that implement those steps. The missing piece is a way to link the two. This is where the Pipeline Configuration comes in. The Pipeline Configuration uses a groovy-based configuration language to ensure the Pipeline Template uses the correct tools and settings for the application. For example, here's a Pipeline Configuration that specifies the npm library should be loaded. // file: pipeline_config.groovy libraries { npm } When a pipeline using JTE runs with this configuration and template, the steps from the npm library are loaded. This means that the unit_test and build steps within the template will use the unit_test and build definitions provided by the npm library! By swapping out Pipeline Configurations , a single Pipeline Template can be used across multiple teams, supporting multiple tech stacks . Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Bottom-Up Explanation"},{"location":"concepts/framework-overview/bottom-up/#bottom-up-explanation","text":"Welcome ! This page is going to take you on a journey from how pipelines are typically built today (on a per-application basis), pausing to look at the challenges that causes at scale, and then step through together how the Jenkins Templating Engine (JTE) works to remediate those pain points. This will be one of the lengthier pages in the documentation. If you can stick to it, you'll come out the other side seeing pipeline development differently.","title":"Bottom-Up Explanation"},{"location":"concepts/framework-overview/bottom-up/#writing-pipelines-without-the-jenkins-templating-engine","text":"Imagine that there are three applications that each need a pipeline to automate the execution of unit tests and building an artifact. Click through the tabs below to see a pipeline for a Gradle application, a Maven application, and an NPM application. Gradle // a basic Gradle pipeline stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } stage ( \"Build\" ){ node { sh \"gradle build\" } } Maven // a basic Maven pipeline stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } stage ( \"Build\" ){ node { sh \"mvn build\" } } NPM // a basic NPM pipeline stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } Traditionally, these pipelines would be stored alongside the source code for the application.","title":"Writing Pipelines Without the Jenkins Templating Engine"},{"location":"concepts/framework-overview/bottom-up/#why-this-approach-doesnt-scale","text":"Defining pipelines on a per-application basis works when you have a small number of teams you're supporting . Over time, though, \"DevOps Teams\" at organizations find themselves responsible for administering a growing number of internal tools and scaling the adoption of those tools across application teams. Why is creating bespoke pipelines such a big deal? It doesn't scale. Individual pipelines means work needs to be done to integrate each application with a pipeline Duplicated pipelines introduce uncertainty that common processes are followed Complexity becomes difficult to manage","title":"Why This Approach Doesn't Scale"},{"location":"concepts/framework-overview/bottom-up/#onboarding-each-team-individually","text":"When pipelines are built on a per-application basis, it leaves organizations with two choices. Either you need a developer on every team who knows how to write pipelines aligned with organizational standards, or you need a dedicated pipeline team onboarding application teams to a pipeline. The first choice often isn't super viable - developers should focus on the problem that they're best suited to solve: developing applications. Even if all the software developers could write their own automated pipelines, it becomes very challenging to ensure these pipelines follow required compliance checks. The second choice requires that you scale the dedicated pipeline team to meet the number of application teams that need support. This is often prohibitively expensive.","title":"Onboarding Each Team Individually"},{"location":"concepts/framework-overview/bottom-up/#standardization-compliance","text":"When pipelines are built on a per-application basis, it becomes extremely challenging to be confident that teams throughout the organization are following approved software delivery processes aligned with cyber compliance requirements. Furthermore, for more tightly governed environments, the idea of a developer being able to modify the Jenkinsfile on their branch to perform a deployment to production is a threat vector that introduces unacceptable, unnecessary risks. Mitigating this risk requires very mature monitoring and correlation across systems.","title":"Standardization &amp; Compliance"},{"location":"concepts/framework-overview/bottom-up/#complexity","text":"There's an old adage, \"running one container is easy, running many is really, really hard.\" The same applies for pipelines within an organization. Creating a pipeline that runs unit tests, builds artifacts, deploys those artifacts, and runs security scans gets exponentially more complex the more teams and technical stacks need to be supported.","title":"Complexity"},{"location":"concepts/framework-overview/bottom-up/#a-better-way-pipeline-templating","text":"These challenges are all caused by building pipelines on a per-application basis . Even if you're modularizing your pipeline code for reuse through Jenkins Shared Libraries, you still end up duplicating the Jenkinsfile across every branch of every source code repository. Warning Don't even get me started on how tricky it can be to propagate a change to the pipeline across teams when the representation of the pipeline lives across each branch independently. Taking a step back, you may have noticed that each of the three pipelines above follow the same structure. First they execute unit tests, then they perform a build. In our experience, this simple pattern holds true most of the time ( especially when working with microservices). While the specific tools that get used to perform a step of the pipeline may change, the workflow remains the same.","title":"A Better Way: Pipeline Templating"},{"location":"concepts/framework-overview/bottom-up/#writing-a-pipeline-template","text":"The entire philosophy behind the Jenkins Template Engine stems from the concept of common workflows with interchangeable tools. What if it was possible to translate the three separate pipelines above into the following Pipeline Template: // file: Jenkinsfile unit_test () build () Good news, this is exactly what JTE makes possible! Next, you'll need to define your implementations of the unit_test() and build() steps.","title":"Writing a Pipeline Template"},{"location":"concepts/framework-overview/bottom-up/#writing-pipeline-steps","text":"Implement the unit_test() and build() steps by refactoring the original pipelines above. Gradle: unit_test // file: gradle/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } } Gradle: build // file: gradle/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"gradle build\" } } } Maven: unit_test // file: maven/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } } Maven: build // file: maven/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"mvn build\" } } } NPM: unit_test // file: npm/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } } NPM: build // file: npm/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } } Note These steps are the exact same pipeline code we had written at the start. We just wrapped it in a call method for Reasons you can learn about over on the library steps page.","title":"Writing Pipeline Steps"},{"location":"concepts/framework-overview/bottom-up/#understanding-libraries","text":"If you go back and look at the comments indicating the files those steps are placed in, you'll notice the following file structure: . \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u2514\u2500\u2500 npm \u2514\u2500\u2500 steps \u251c\u2500\u2500 build.groovy \u2514\u2500\u2500 unit_test.groovy A library in JTE is a collection of steps (stored together in a directory) that can be loaded at runtime. Earlier, a common Pipeline Template was defined that executes unit tests and then builds an artifact. This template can be shared across teams. Then, three libraries were created: npm , gradle , and maven . Each of these libraries implemented the steps required by the template. Finally, JTE needs a way to determine which libraries to load for a given team's pipeline.","title":"Understanding Libraries"},{"location":"concepts/framework-overview/bottom-up/#pipeline-configuration","text":"So far, you've defined a Pipeline Template that invokes steps, and libraries that implement those steps. The missing piece is a way to link the two. This is where the Pipeline Configuration comes in. The Pipeline Configuration uses a groovy-based configuration language to ensure the Pipeline Template uses the correct tools and settings for the application. For example, here's a Pipeline Configuration that specifies the npm library should be loaded. // file: pipeline_config.groovy libraries { npm } When a pipeline using JTE runs with this configuration and template, the steps from the npm library are loaded. This means that the unit_test and build steps within the template will use the unit_test and build definitions provided by the npm library! By swapping out Pipeline Configurations , a single Pipeline Template can be used across multiple teams, supporting multiple tech stacks . Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Pipeline Configuration"},{"location":"concepts/framework-overview/key-benefits/","text":"Key Benefits \u00b6 Here's a distilled explanation of why you should use JTE. JTE is a pipeline development framework for creating tool-agnostic, templated workflows that can be shared across teams creating applications with different technologies. This approach separates the business logic ( Pipeline Template ) from the technical implementation ( Pipeline Primitives ) allowing teams to configure their pipelines rather than build them from scratch. Business Value Organizational Governance The elements of Pipeline Governance in JTE allow organizations to scale DevSecOps and have assurances that required security gates are being performed. Optimize Pipeline Code Reuse The plug-and-play nature of Pipeline Primitives helps keep Pipeline Templates DRY. Simplify Pipeline Maintainability When managing more than a couple pipelines, it's simpler to manage a set of reusable Pipeline Templates in a Pipeline Catalog with modularized Pipeline Libraries than it is to copy and paste a Jenkinsfile into every repository and tweak it for the new application.","title":"Key Benefits"},{"location":"concepts/framework-overview/key-benefits/#key-benefits","text":"Here's a distilled explanation of why you should use JTE. JTE is a pipeline development framework for creating tool-agnostic, templated workflows that can be shared across teams creating applications with different technologies. This approach separates the business logic ( Pipeline Template ) from the technical implementation ( Pipeline Primitives ) allowing teams to configure their pipelines rather than build them from scratch. Business Value Organizational Governance The elements of Pipeline Governance in JTE allow organizations to scale DevSecOps and have assurances that required security gates are being performed. Optimize Pipeline Code Reuse The plug-and-play nature of Pipeline Primitives helps keep Pipeline Templates DRY. Simplify Pipeline Maintainability When managing more than a couple pipelines, it's simpler to manage a set of reusable Pipeline Templates in a Pipeline Catalog with modularized Pipeline Libraries than it is to copy and paste a Jenkinsfile into every repository and tweak it for the new application.","title":"Key Benefits"},{"location":"concepts/framework-overview/overview/","text":"Overview \u00b6 Concept pages are understanding oriented articles describing how JTE works. The Framework Overview section is the best place to get started building your mental model of how JTE works. Getting Started \u00b6 Everyone learns differently. The following pages explain JTE from different perspectives. Page Description Bottom-Up Explanation Learn JTE by transforming a set of existing Jenkins Pipelines into a reusable Pipeline Template Top-Down Learn JTE by diving right in from the perspective of how JTE works Key Benefits Learn how to articulate the value of Pipeline Templating Introductory Webinar \u00b6 If you learn best by watching videos, this Continuous Delivery Foundation webinar is a great place to start. Learn More \u00b6 After reviewing the framework overviews, check out the following sections to start learning more. Section Description Pipeline Templates Learn more about Pipeline Templates and how to define workflows Pipeline Configuration Learn more about Pipeline Configurations Pipeline Primitives Learn more about Pipeline Primitives and how to make Pipeline Templates reusable Library Development Learn more about Library Development and how to write Library Steps Pipeline Governance Learn more about how to govern template selection, library resolution, and Pipeline Configuration settings Advanced Topics Learn about advanced features that don't cleanly fit anywhere else","title":"Overview"},{"location":"concepts/framework-overview/overview/#overview","text":"Concept pages are understanding oriented articles describing how JTE works. The Framework Overview section is the best place to get started building your mental model of how JTE works.","title":"Overview"},{"location":"concepts/framework-overview/overview/#getting-started","text":"Everyone learns differently. The following pages explain JTE from different perspectives. Page Description Bottom-Up Explanation Learn JTE by transforming a set of existing Jenkins Pipelines into a reusable Pipeline Template Top-Down Learn JTE by diving right in from the perspective of how JTE works Key Benefits Learn how to articulate the value of Pipeline Templating","title":"Getting Started"},{"location":"concepts/framework-overview/overview/#introductory-webinar","text":"If you learn best by watching videos, this Continuous Delivery Foundation webinar is a great place to start.","title":"Introductory Webinar"},{"location":"concepts/framework-overview/overview/#learn-more","text":"After reviewing the framework overviews, check out the following sections to start learning more. Section Description Pipeline Templates Learn more about Pipeline Templates and how to define workflows Pipeline Configuration Learn more about Pipeline Configurations Pipeline Primitives Learn more about Pipeline Primitives and how to make Pipeline Templates reusable Library Development Learn more about Library Development and how to write Library Steps Pipeline Governance Learn more about how to govern template selection, library resolution, and Pipeline Configuration settings Advanced Topics Learn about advanced features that don't cleanly fit anywhere else","title":"Learn More"},{"location":"concepts/framework-overview/top-down/","text":"Top-Down Explanation \u00b6 Welcome ! This explanation is best suited for more experienced Jenkins users that are familiar with Jenkins pipeline's scripted syntax or software developers familiar with software design patterns. Overview \u00b6 To put it simply, the problem JTE is trying to solve is: How can organizations stop building pipelines for each application individually? The answer comes from the idea that within an organization, software development processes can be distilled into a subset of generic workflows. Regardless of which tools are being used, the process often says the same. Teams are typically going to run unit tests, build a software artifact, scan it, deploy it somewhere, test it some more, and promote that artifact to higher Application Environments. Some teams do more, some teams do less, but it doesn't matter if that process uses npm , sonarqube , docker , and helm or gradle , fortify , and ansible ; the process is the same. As depicted in Figure 1, JTE allows you to take that process and represent it as a tool-agnostic Pipeline Template. This abstract Pipeline Template can then be made concrete by loading Pipeline Primitives such as steps. Which Pipeline Primitives to inject are determined by a Pipeline Configuration. Figure 1 Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected. Pipeline Templates \u00b6 A Pipeline Template is nothing more than a Jenkinsfile with some stuff added at runtime. Everything that can be done in a Jenkinsfile can be done in a Pipeline Template. The only thing that makes a template a template is the use of Pipeline Primitives such that a single template can be used for multiple teams and multiple tools. A visualization of a Pipeline Template with steps being swapped in an out Pipeline Primitives \u00b6 Pipeline Primitives are contributed by the JTE framework and help make templates reusable. Pipeline Templates will typically make use of identically named Pipeline Primitives, such as step called build() , to become reusable. Pipeline Configuration \u00b6 The Pipeline Configuration is where teams declare which Pipeline Primitives should be injected into the Pipeline Template for their application(s).","title":"Top-Down Explanation"},{"location":"concepts/framework-overview/top-down/#top-down-explanation","text":"Welcome ! This explanation is best suited for more experienced Jenkins users that are familiar with Jenkins pipeline's scripted syntax or software developers familiar with software design patterns.","title":"Top-Down Explanation"},{"location":"concepts/framework-overview/top-down/#overview","text":"To put it simply, the problem JTE is trying to solve is: How can organizations stop building pipelines for each application individually? The answer comes from the idea that within an organization, software development processes can be distilled into a subset of generic workflows. Regardless of which tools are being used, the process often says the same. Teams are typically going to run unit tests, build a software artifact, scan it, deploy it somewhere, test it some more, and promote that artifact to higher Application Environments. Some teams do more, some teams do less, but it doesn't matter if that process uses npm , sonarqube , docker , and helm or gradle , fortify , and ansible ; the process is the same. As depicted in Figure 1, JTE allows you to take that process and represent it as a tool-agnostic Pipeline Template. This abstract Pipeline Template can then be made concrete by loading Pipeline Primitives such as steps. Which Pipeline Primitives to inject are determined by a Pipeline Configuration. Figure 1 Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Overview"},{"location":"concepts/framework-overview/top-down/#pipeline-templates","text":"A Pipeline Template is nothing more than a Jenkinsfile with some stuff added at runtime. Everything that can be done in a Jenkinsfile can be done in a Pipeline Template. The only thing that makes a template a template is the use of Pipeline Primitives such that a single template can be used for multiple teams and multiple tools. A visualization of a Pipeline Template with steps being swapped in an out","title":"Pipeline Templates"},{"location":"concepts/framework-overview/top-down/#pipeline-primitives","text":"Pipeline Primitives are contributed by the JTE framework and help make templates reusable. Pipeline Templates will typically make use of identically named Pipeline Primitives, such as step called build() , to become reusable.","title":"Pipeline Primitives"},{"location":"concepts/framework-overview/top-down/#pipeline-configuration","text":"The Pipeline Configuration is where teams declare which Pipeline Primitives should be injected into the Pipeline Template for their application(s).","title":"Pipeline Configuration"},{"location":"concepts/framework-overview/snippets/design-patterns/","text":"Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Design patterns"},{"location":"concepts/framework-overview/snippets/getting-started/","text":"banana \u00b6","title":"banana"},{"location":"concepts/framework-overview/snippets/getting-started/#banana","text":"","title":"banana"},{"location":"concepts/library-development/library-classes/","text":"Library Classes \u00b6 Libraries have the option to provide groovy classes. These classes should be placed in the src directory of the library. Tip Each loaded library's src directory contents are synchronized to a common directory. Take care not to load two libraries that provide the same class. One way to avoid this is to name the package after the contributing library. For example, if the library's name was example then put the library's classes in the src/example/ directory with package example at the top of the class files. Class Serializability \u00b6 Library classes should implement the Serializable interface whenever possible. For example, a Utility class coming from an example library: package example class Utility implements Serializable {} This is because Jenkins pipeline's implement a design pattern called Continuation Passing Style (CPS) so that individual Pipeline Runs can resume progress. Note To learn more, check out Best Practices for Avoiding Serializability Exceptions Classpath \u00b6 Classes contributed by loaded libraries can be imported from the library's steps, steps from other libraries, and the Pipeline Template. Warning Importing a library class from a Pipeline Template or from a step outside the library will lead to tight coupling. In general, library classes should be utilized within steps from the same library Jenkins Step Resolution \u00b6 Library classes can not resolve Jenkins pipeline DSL functions such as sh or echo . A work around for this is to pass the steps variable to the class constructor to store on a field or through a method parameter. For example, to use the echo pipeline step one could do the following: Utility Class package example class Utility implements Serializable { void doThing ( steps ){ steps . echo \"message from the Utility class\" } } Library Step import example.Utility void call (){ Utility u = new Utility () u . doThing ( steps ) } Accessing the Library Configuration \u00b6 Unlike with library steps, the config and pipelineConfig variables aren't autowired to library classes. To access these variables, they can be passed to the class through constructor or method parameters. Utility Class package example class Utility implements Serializable { def config Utility ( config ){ this . config = config } void doThing ( steps ){ steps . echo \"library config: ${config}\" } } Library Step import example.Utility void call (){ Utility u = new Utility ( config ) u . doThing ( steps ) }","title":"Library Classes"},{"location":"concepts/library-development/library-classes/#library-classes","text":"Libraries have the option to provide groovy classes. These classes should be placed in the src directory of the library. Tip Each loaded library's src directory contents are synchronized to a common directory. Take care not to load two libraries that provide the same class. One way to avoid this is to name the package after the contributing library. For example, if the library's name was example then put the library's classes in the src/example/ directory with package example at the top of the class files.","title":"Library Classes"},{"location":"concepts/library-development/library-classes/#class-serializability","text":"Library classes should implement the Serializable interface whenever possible. For example, a Utility class coming from an example library: package example class Utility implements Serializable {} This is because Jenkins pipeline's implement a design pattern called Continuation Passing Style (CPS) so that individual Pipeline Runs can resume progress. Note To learn more, check out Best Practices for Avoiding Serializability Exceptions","title":"Class Serializability"},{"location":"concepts/library-development/library-classes/#classpath","text":"Classes contributed by loaded libraries can be imported from the library's steps, steps from other libraries, and the Pipeline Template. Warning Importing a library class from a Pipeline Template or from a step outside the library will lead to tight coupling. In general, library classes should be utilized within steps from the same library","title":"Classpath"},{"location":"concepts/library-development/library-classes/#jenkins-step-resolution","text":"Library classes can not resolve Jenkins pipeline DSL functions such as sh or echo . A work around for this is to pass the steps variable to the class constructor to store on a field or through a method parameter. For example, to use the echo pipeline step one could do the following: Utility Class package example class Utility implements Serializable { void doThing ( steps ){ steps . echo \"message from the Utility class\" } } Library Step import example.Utility void call (){ Utility u = new Utility () u . doThing ( steps ) }","title":"Jenkins Step Resolution"},{"location":"concepts/library-development/library-classes/#accessing-the-library-configuration","text":"Unlike with library steps, the config and pipelineConfig variables aren't autowired to library classes. To access these variables, they can be passed to the class through constructor or method parameters. Utility Class package example class Utility implements Serializable { def config Utility ( config ){ this . config = config } void doThing ( steps ){ steps . echo \"library config: ${config}\" } } Library Step import example.Utility void call (){ Utility u = new Utility ( config ) u . doThing ( steps ) }","title":"Accessing the Library Configuration"},{"location":"concepts/library-development/library-configuration-file/","text":"Library Configuration File \u00b6 The root of a library can contain an optional library_config.groovy file that captures metadata about the library. Library Parameter Validation \u00b6 Currently, the library configuration file is only used to validate library configurations. Reference A comprehensive overview of the library configuration schema can be found in the Reference section. Advanced Library Validations \u00b6 For library parameter validations that more complex than what can be accomplished through the library configuration functionality, library developers can alternatively create a step annotated with the @Validate Lifecycle Hook . Methods within steps annotated with @Validate will execute before the Pipeline Template. For example, if a library wanted to validate a more complex use case such as ensuring a library parameter named threshold was greater than or equal to zero but less than or equal to 100 the following could be implemented: @Validate [ 1 ] void call ( context ){ [ 2 ] if ( config . threshold < 0 || config . threshold > 100 ){ [ 3 ] error \"Library parameter 'threshold' must be within the range of: 0 <= threshold <= 100\" [ 4 ] } } The @Validate annotation marks a method defined within a step to be invoked before template execution. This example defines a call() method, but the method name can be any valid Groovy method name. Here, a Groovy if statement is used to validate that the threshold parameter fall within a certain range. If the threshold variable doesn't meet the criteria, the Jenkins pipeline error step is used to fail the build. The warning step could also be used if the pipeline user should be notified but the build should continue. This approach allows library developers to use Groovy to validate arbitrarily complex library parameter constraints. The method annotated with @Validate can be in its own step file or added as an additional method within an existing step file. Note The example above assumes that the threshold library parameter has been configured as part of the Pipeline Configuration. This could be also be validated using Groovy or by combining the functionality of the library configuration file to set the threshold parameter as a required field that must be a Number.","title":"Library Configuration File"},{"location":"concepts/library-development/library-configuration-file/#library-configuration-file","text":"The root of a library can contain an optional library_config.groovy file that captures metadata about the library.","title":"Library Configuration File"},{"location":"concepts/library-development/library-configuration-file/#library-parameter-validation","text":"Currently, the library configuration file is only used to validate library configurations. Reference A comprehensive overview of the library configuration schema can be found in the Reference section.","title":"Library Parameter Validation"},{"location":"concepts/library-development/library-configuration-file/#advanced-library-validations","text":"For library parameter validations that more complex than what can be accomplished through the library configuration functionality, library developers can alternatively create a step annotated with the @Validate Lifecycle Hook . Methods within steps annotated with @Validate will execute before the Pipeline Template. For example, if a library wanted to validate a more complex use case such as ensuring a library parameter named threshold was greater than or equal to zero but less than or equal to 100 the following could be implemented: @Validate [ 1 ] void call ( context ){ [ 2 ] if ( config . threshold < 0 || config . threshold > 100 ){ [ 3 ] error \"Library parameter 'threshold' must be within the range of: 0 <= threshold <= 100\" [ 4 ] } } The @Validate annotation marks a method defined within a step to be invoked before template execution. This example defines a call() method, but the method name can be any valid Groovy method name. Here, a Groovy if statement is used to validate that the threshold parameter fall within a certain range. If the threshold variable doesn't meet the criteria, the Jenkins pipeline error step is used to fail the build. The warning step could also be used if the pipeline user should be notified but the build should continue. This approach allows library developers to use Groovy to validate arbitrarily complex library parameter constraints. The method annotated with @Validate can be in its own step file or added as an additional method within an existing step file. Note The example above assumes that the threshold library parameter has been configured as part of the Pipeline Configuration. This could be also be validated using Groovy or by combining the functionality of the library configuration file to set the threshold parameter as a required field that must be a Number.","title":"Advanced Library Validations"},{"location":"concepts/library-development/library-resources/","text":"Library Resources \u00b6 Libraries can store static assets, such as shell scripts or YAML files, in a resources directory. Accessing A Library Resource \u00b6 Within a Library Step , the resource(String relativePath) method can be used to return the file contents of a resource as a String . Example In the following example, a Library Step is created that executes a script from the root of the resources directory and then reads in data from a YAML file nested within the resources directory. Library Structure exampleLibraryName \u251c\u2500\u2500 steps \u2502 \u2514\u2500\u2500 step.groovy \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 doSomething.sh \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 data.yaml \u2514\u2500\u2500 library_config.groovy step.groovy void call (){ String script = resource ( \"doSomething.sh\" ) def data = readYaml text: resource ( \"nested/data.yaml\" ) } Important The path parameter passed to the resource method must be a relative path within the resources directory Only steps within a library can access the library's resources (no cross-library resource fetching) The resource() method is only available within Library Steps and can't be invoked from the Pipeline Template","title":"Library Resources"},{"location":"concepts/library-development/library-resources/#library-resources","text":"Libraries can store static assets, such as shell scripts or YAML files, in a resources directory.","title":"Library Resources"},{"location":"concepts/library-development/library-resources/#accessing-a-library-resource","text":"Within a Library Step , the resource(String relativePath) method can be used to return the file contents of a resource as a String . Example In the following example, a Library Step is created that executes a script from the root of the resources directory and then reads in data from a YAML file nested within the resources directory. Library Structure exampleLibraryName \u251c\u2500\u2500 steps \u2502 \u2514\u2500\u2500 step.groovy \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 doSomething.sh \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 data.yaml \u2514\u2500\u2500 library_config.groovy step.groovy void call (){ String script = resource ( \"doSomething.sh\" ) def data = readYaml text: resource ( \"nested/data.yaml\" ) } Important The path parameter passed to the resource method must be a relative path within the resources directory Only steps within a library can access the library's resources (no cross-library resource fetching) The resource() method is only available within Library Steps and can't be invoked from the Pipeline Template","title":"Accessing A Library Resource"},{"location":"concepts/library-development/library-source/","text":"Library Source \u00b6 A Library Source is a reference to a location where one or more libraries can be fetched. Library Source Structure \u00b6 Within a configured Library Source, a library is a directory . The name of the directory is the identifier that would be declared in the libraries{} block of the Pipeline Configuration. Library Providers \u00b6 The Jenkins Templating Engine plugin provides an interface to create Library Sources. The plugin comes with two types of Library Sources built-in: SCM Library Sources and Plugin Library Sources. SCM Library Source \u00b6 The SCM Library Source is used to fetch libraries from a source code repository. This repository can be a local directory with a .git directory accessible from the Jenkins Controller or a remote repository. Plugin Library Source \u00b6 The Plugin Library Source is used when libraries have been bundled into a separate plugin. This option will only be available in the Jenkins UI when a plugin has been installed that can serve as a library-providing plugin.","title":"Library Source"},{"location":"concepts/library-development/library-source/#library-source","text":"A Library Source is a reference to a location where one or more libraries can be fetched.","title":"Library Source"},{"location":"concepts/library-development/library-source/#library-source-structure","text":"Within a configured Library Source, a library is a directory . The name of the directory is the identifier that would be declared in the libraries{} block of the Pipeline Configuration.","title":"Library Source Structure"},{"location":"concepts/library-development/library-source/#library-providers","text":"The Jenkins Templating Engine plugin provides an interface to create Library Sources. The plugin comes with two types of Library Sources built-in: SCM Library Sources and Plugin Library Sources.","title":"Library Providers"},{"location":"concepts/library-development/library-source/#scm-library-source","text":"The SCM Library Source is used to fetch libraries from a source code repository. This repository can be a local directory with a .git directory accessible from the Jenkins Controller or a remote repository.","title":"SCM Library Source"},{"location":"concepts/library-development/library-source/#plugin-library-source","text":"The Plugin Library Source is used when libraries have been bundled into a separate plugin. This option will only be available in the Jenkins UI when a plugin has been installed that can serve as a library-providing plugin.","title":"Plugin Library Source"},{"location":"concepts/library-development/library-steps/","text":"Library Steps \u00b6 Library Steps are a mechanism for modularizing pipeline functionality. Naming A Step \u00b6 By default, the name of the step that's loaded is based on the filename without the .groovy extension. This can be modified using Step Aliasing . Best Practice It's recommended that Step Aliasing only be used when actually necessary. The call Method \u00b6 Most steps should implement the call method. void call (){} This makes it such that the step can be invoked via its name. For example, a step named build.groovy that has implemented a call method can be invoked via build() . Why the Call Method? Curious readers commonly ask, \"Why the call method?\" The answer comes from the Groovy Call Operator . Essentially, build() is equivalent to build.call() in groovy. Autowired Variables \u00b6 All Library Steps are autowired with several variables: Variable Description config The library's block of configuration for the library that contributed the step. stepContext Information about the step that's currently running. stageContext Information about the current Stage , if applicable hookContext If this step was triggered by a Lifecycle Hook , information about the trigger Reference For more information, check out the Autowired Variables page Method Parameters \u00b6 Library Steps can accept method parameters just like any other method. Library Step Method Parameters printMessage.groovy void call ( String message ){ println \"here's your message: ${message}\" } Step Invocation printMessage ( \"hello, world!\" ) Be Careful! Library Steps that accept method parameters run a high risk of breaking the interoperability of the Pipeline Template . Imagine the scenario where the Pipeline Template invokes a build() step and the same template is intended to be used across teams that may be using different tools, such as gradle and npm . If the gradle library's build() step accepts a set of parameters and the npm library's build() step doesn't then you won't be able to swap out the libraries interchangeably. Instead of method parameters, consider passing steps information via the Pipeline Configuration using the config variable. Check out the Parameterizing Libraries page to learn more. The exception to the rule of thumb regarding method parameters is when the method parameters are Pipeline Primitives. This works because the parameters can then be interchanged safely along with the implementation of the step that's accepting them as an argument. The most common example is creating a deployment step. Frequently, teams will create a deploy_to step that accepts an Application Environment as an argument. Deployment Steps The following Pipeline Template, Pipeline Configuration, and Deployment step demonstrate a safe use of a library step accepting a method parameter. Pipeline Template unit_test () build () deploy_to dev smoke_test () deploy_to prod Pipeline Configuration libraries { npm // contributes unit_test, build cypress // contributes integration_test ansible // contributes deploy_to } application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Deployment Step // ansible/steps/deploy_to.groovy void call ( app_env ){ println \"deploying to the ip: ${app_env.ip}\" } Advanced Topics \u00b6 This page has covered the basics, if you're ready for more check out the following pages: Topic Description Lifecycle Hooks Learn how to trigger Library Steps implicitly. Multi-Method Library Steps Learn how to define more than one method in a step. Step Aliasing Learn how to call the same step by multiple names.","title":"Library Steps"},{"location":"concepts/library-development/library-steps/#library-steps","text":"Library Steps are a mechanism for modularizing pipeline functionality.","title":"Library Steps"},{"location":"concepts/library-development/library-steps/#naming-a-step","text":"By default, the name of the step that's loaded is based on the filename without the .groovy extension. This can be modified using Step Aliasing . Best Practice It's recommended that Step Aliasing only be used when actually necessary.","title":"Naming A Step"},{"location":"concepts/library-development/library-steps/#the-call-method","text":"Most steps should implement the call method. void call (){} This makes it such that the step can be invoked via its name. For example, a step named build.groovy that has implemented a call method can be invoked via build() . Why the Call Method? Curious readers commonly ask, \"Why the call method?\" The answer comes from the Groovy Call Operator . Essentially, build() is equivalent to build.call() in groovy.","title":"The call Method"},{"location":"concepts/library-development/library-steps/#autowired-variables","text":"All Library Steps are autowired with several variables: Variable Description config The library's block of configuration for the library that contributed the step. stepContext Information about the step that's currently running. stageContext Information about the current Stage , if applicable hookContext If this step was triggered by a Lifecycle Hook , information about the trigger Reference For more information, check out the Autowired Variables page","title":"Autowired Variables"},{"location":"concepts/library-development/library-steps/#method-parameters","text":"Library Steps can accept method parameters just like any other method. Library Step Method Parameters printMessage.groovy void call ( String message ){ println \"here's your message: ${message}\" } Step Invocation printMessage ( \"hello, world!\" ) Be Careful! Library Steps that accept method parameters run a high risk of breaking the interoperability of the Pipeline Template . Imagine the scenario where the Pipeline Template invokes a build() step and the same template is intended to be used across teams that may be using different tools, such as gradle and npm . If the gradle library's build() step accepts a set of parameters and the npm library's build() step doesn't then you won't be able to swap out the libraries interchangeably. Instead of method parameters, consider passing steps information via the Pipeline Configuration using the config variable. Check out the Parameterizing Libraries page to learn more. The exception to the rule of thumb regarding method parameters is when the method parameters are Pipeline Primitives. This works because the parameters can then be interchanged safely along with the implementation of the step that's accepting them as an argument. The most common example is creating a deployment step. Frequently, teams will create a deploy_to step that accepts an Application Environment as an argument. Deployment Steps The following Pipeline Template, Pipeline Configuration, and Deployment step demonstrate a safe use of a library step accepting a method parameter. Pipeline Template unit_test () build () deploy_to dev smoke_test () deploy_to prod Pipeline Configuration libraries { npm // contributes unit_test, build cypress // contributes integration_test ansible // contributes deploy_to } application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Deployment Step // ansible/steps/deploy_to.groovy void call ( app_env ){ println \"deploying to the ip: ${app_env.ip}\" }","title":"Method Parameters"},{"location":"concepts/library-development/library-steps/#advanced-topics","text":"This page has covered the basics, if you're ready for more check out the following pages: Topic Description Lifecycle Hooks Learn how to trigger Library Steps implicitly. Multi-Method Library Steps Learn how to define more than one method in a step. Step Aliasing Learn how to call the same step by multiple names.","title":"Advanced Topics"},{"location":"concepts/library-development/library-structure/","text":"Library Structure \u00b6 Overview \u00b6 Each directory within a Library Source is a different library that can be loaded via the Pipeline Configuration The name of the directory is the library identifier used within the Pipeline Configuration libraries{} block when loading the library. Path Description steps/**/*.groovy groovy files under the steps directory will be loaded as steps where the basename of the file will be the name of the function made available in the pipeline resources/**/* any file under the resources directory will be accessible from within Library Steps via the resource() step src/**/* Classes contributed by the library that can be imported from within Pipeline Templates and steps library_config.groovy the library configuration file Example Library Structure \u00b6 exampleLibraryName [1] \u251c\u2500\u2500 steps [2] \u2502 \u2514\u2500\u2500 step1.groovy [3] \u2502 \u2514\u2500\u2500 step2.groovy \u251c\u2500\u2500 resources [4] \u2502 \u251c\u2500\u2500 someResource.txt [5] \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 anotherResource.json [5] \u251c\u2500\u2500 src [7] \u2502 \u2514\u2500\u2500 example \u2502 \u2514\u2500\u2500 Utility.groovy [8] \u2514\u2500\u2500 library_config.groovy [9] This library would be loaded via the exampleLibraryName identifier in the libraries{} block All steps contributed by the library goes in the steps directory An example step. A step1 step would be added to the pipeline All library resources go in the resources directory A root level resource. The contents could be fetched from step1 or step2 via resource(\"someResource.txt\") A nested resource. The contents could be fetched from step1 or step2 via resource(\"nested/anotherResource.json\") File paths within the src directory must be unique across libraries loaded and will be made available to the Class Loader for both steps and templates A class file containing the example.Utility class. The library configuration file","title":"Library Structure"},{"location":"concepts/library-development/library-structure/#library-structure","text":"","title":"Library Structure"},{"location":"concepts/library-development/library-structure/#overview","text":"Each directory within a Library Source is a different library that can be loaded via the Pipeline Configuration The name of the directory is the library identifier used within the Pipeline Configuration libraries{} block when loading the library. Path Description steps/**/*.groovy groovy files under the steps directory will be loaded as steps where the basename of the file will be the name of the function made available in the pipeline resources/**/* any file under the resources directory will be accessible from within Library Steps via the resource() step src/**/* Classes contributed by the library that can be imported from within Pipeline Templates and steps library_config.groovy the library configuration file","title":"Overview"},{"location":"concepts/library-development/library-structure/#example-library-structure","text":"exampleLibraryName [1] \u251c\u2500\u2500 steps [2] \u2502 \u2514\u2500\u2500 step1.groovy [3] \u2502 \u2514\u2500\u2500 step2.groovy \u251c\u2500\u2500 resources [4] \u2502 \u251c\u2500\u2500 someResource.txt [5] \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 anotherResource.json [5] \u251c\u2500\u2500 src [7] \u2502 \u2514\u2500\u2500 example \u2502 \u2514\u2500\u2500 Utility.groovy [8] \u2514\u2500\u2500 library_config.groovy [9] This library would be loaded via the exampleLibraryName identifier in the libraries{} block All steps contributed by the library goes in the steps directory An example step. A step1 step would be added to the pipeline All library resources go in the resources directory A root level resource. The contents could be fetched from step1 or step2 via resource(\"someResource.txt\") A nested resource. The contents could be fetched from step1 or step2 via resource(\"nested/anotherResource.json\") File paths within the src directory must be unique across libraries loaded and will be made available to the Class Loader for both steps and templates A class file containing the example.Utility class. The library configuration file","title":"Example Library Structure"},{"location":"concepts/library-development/lifecycle-hooks/","text":"Lifecycle Hooks \u00b6 Sometimes it's necessary to trigger specific pipeline actions at certain times during pipeline execution. For example, if you wanted to send multiple notification types after a particular pipeline step or at the conclusion of a pipeline if the build was failure. JTE supports this type of Aspect Oriented Programming style event handling through annotation markers that can be placed on methods defined within Library Steps. Hook Types \u00b6 The following lifecycle hook annotations are available: Annotation Trigger @Validate Beginning of a Pipeline Run, before the Pipeline Template @Init After all @Validate hooks, before the Pipeline Template @BeforeStep During template execution, before every Library Step @AfterStep During template execution, after every Library Step @CleanUp After template execution @Notify During template execution after every Library Step and after template execution Hook Context \u00b6 Lifecycle Hook annotations can be placed on any method inside a step. Every step has an autowired hookContext variable which provides steps with relevant information about what triggered the hook. Property Description hookContext.library The name of the library that contributed the step associated with the step. Value is null for hooks not triggered by a step. hookContext.step The name of the Library Step that triggered the hook. Value is null for hooks not triggered by a step. Conditional Hook Execution \u00b6 Sometimes you'll only want to invoke the Hook when certain conditions are met, such as a build failure or in relation to another step (like before static code analysis). Each annotation accepts a Closure parameter. If the return object of this closure is http://www.groovy-lang.org/semantics.html#Groovy-Truth[truthy ] then the hook will be executed. While executing, the code within the Closure parameter will be able to resolve the hookContext variable, the library configuration of the library that loads the step via the config variable, and the currentBuild variable made available in Jenkins Pipelines. Example Syntax: @BeforeStep ({ hookContext . step . equals ( \"build\" ) }) void call (){ // execute something right before the Library Step called build is executed. } Note The closure parameter is optional. If omitted, the hook will always be executed.","title":"Lifecycle Hooks"},{"location":"concepts/library-development/lifecycle-hooks/#lifecycle-hooks","text":"Sometimes it's necessary to trigger specific pipeline actions at certain times during pipeline execution. For example, if you wanted to send multiple notification types after a particular pipeline step or at the conclusion of a pipeline if the build was failure. JTE supports this type of Aspect Oriented Programming style event handling through annotation markers that can be placed on methods defined within Library Steps.","title":"Lifecycle Hooks"},{"location":"concepts/library-development/lifecycle-hooks/#hook-types","text":"The following lifecycle hook annotations are available: Annotation Trigger @Validate Beginning of a Pipeline Run, before the Pipeline Template @Init After all @Validate hooks, before the Pipeline Template @BeforeStep During template execution, before every Library Step @AfterStep During template execution, after every Library Step @CleanUp After template execution @Notify During template execution after every Library Step and after template execution","title":"Hook Types"},{"location":"concepts/library-development/lifecycle-hooks/#hook-context","text":"Lifecycle Hook annotations can be placed on any method inside a step. Every step has an autowired hookContext variable which provides steps with relevant information about what triggered the hook. Property Description hookContext.library The name of the library that contributed the step associated with the step. Value is null for hooks not triggered by a step. hookContext.step The name of the Library Step that triggered the hook. Value is null for hooks not triggered by a step.","title":"Hook Context"},{"location":"concepts/library-development/lifecycle-hooks/#conditional-hook-execution","text":"Sometimes you'll only want to invoke the Hook when certain conditions are met, such as a build failure or in relation to another step (like before static code analysis). Each annotation accepts a Closure parameter. If the return object of this closure is http://www.groovy-lang.org/semantics.html#Groovy-Truth[truthy ] then the hook will be executed. While executing, the code within the Closure parameter will be able to resolve the hookContext variable, the library configuration of the library that loads the step via the config variable, and the currentBuild variable made available in Jenkins Pipelines. Example Syntax: @BeforeStep ({ hookContext . step . equals ( \"build\" ) }) void call (){ // execute something right before the Library Step called build is executed. } Note The closure parameter is optional. If omitted, the hook will always be executed.","title":"Conditional Hook Execution"},{"location":"concepts/library-development/multi-method-steps/","text":"Multi-Method Library Steps \u00b6 Typically, Library Steps define a call() method that allows the step to be invoked via its name (such as build() ). This isn't required. Groovy's Call Operator 1 means that invoking build() functionally equivalent to invoking build.call() . Steps, therefore, can define alternative methods beyond just the call() method. Use Case: Utility Steps \u00b6 Multi-Method Library Steps are most useful when creating Library Steps that wrap a particular utility. The methods on the step can then represent different actions the utility can take. Utility Step Example: Git Git Utility Step void add ( String files ){ sh \"git add ${files}\" } void commit ( String message ){ sh \"git commit -m ${message}\" } void push (){ sh \"git push\" } Usage ```groovy node{ checkout scm writeFile file: 'test.txt', text: 'hello, world' git.add('test.txt') git.commit('add test file') git.push() } ``` Groovy Command Chain \u00b6 Expressive DSLs can be created when coupling multi-method steps with Groovy's Command Chain feature. Using Command Chains With The Git Utility Command Chains could be used to improve upon the previous example. Without Command Chains node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () } With Command Chains node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git add 'test.txt' git commit 'add test file' git . push () } Groovy Call Operator \u21a9","title":"Multi-Method Library Steps"},{"location":"concepts/library-development/multi-method-steps/#multi-method-library-steps","text":"Typically, Library Steps define a call() method that allows the step to be invoked via its name (such as build() ). This isn't required. Groovy's Call Operator 1 means that invoking build() functionally equivalent to invoking build.call() . Steps, therefore, can define alternative methods beyond just the call() method.","title":"Multi-Method Library Steps"},{"location":"concepts/library-development/multi-method-steps/#use-case-utility-steps","text":"Multi-Method Library Steps are most useful when creating Library Steps that wrap a particular utility. The methods on the step can then represent different actions the utility can take. Utility Step Example: Git Git Utility Step void add ( String files ){ sh \"git add ${files}\" } void commit ( String message ){ sh \"git commit -m ${message}\" } void push (){ sh \"git push\" } Usage ```groovy node{ checkout scm writeFile file: 'test.txt', text: 'hello, world' git.add('test.txt') git.commit('add test file') git.push() } ```","title":"Use Case: Utility Steps"},{"location":"concepts/library-development/multi-method-steps/#groovy-command-chain","text":"Expressive DSLs can be created when coupling multi-method steps with Groovy's Command Chain feature. Using Command Chains With The Git Utility Command Chains could be used to improve upon the previous example. Without Command Chains node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () } With Command Chains node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git add 'test.txt' git commit 'add test file' git . push () } Groovy Call Operator \u21a9","title":"Groovy Command Chain"},{"location":"concepts/library-development/overview/","text":"Overview \u00b6 Pipeline Libraries are the foundation of the Jenkins Templating Engine to allow Pipeline Templates to be shared across teams. Libraries provide steps that can be invoked from templates. Library Repository Scaffold Check out this starter repository to help you get off on the right foot. Loading Libraries \u00b6 The libraries{} block within the Pipeline Configuration defines which libraries will be loaded for a particular Pipeline Run. Learn More \u00b6 Topic Description Library Structure Learn how files are organized within a library. Library Steps Learn how to add steps to a library. Library Resources Learn how to use static assets from within Library Steps Library Classes Learn how to define classes within a library. Parameterizing Libraries Learn how to make libraries configurable from the Pipeline Configuration Library Configuration File Learn how to validate library parameters using the library configuration file Library Sources Learn how to make a library discoverable for loading. Lifecycle Hooks Learn how to register steps for implicit invocation. Multi-Method Library Steps Learn how to define multiple methods per step. Step Aliasing Learn how to invoke the same step by multiple names.","title":"Overview"},{"location":"concepts/library-development/overview/#overview","text":"Pipeline Libraries are the foundation of the Jenkins Templating Engine to allow Pipeline Templates to be shared across teams. Libraries provide steps that can be invoked from templates. Library Repository Scaffold Check out this starter repository to help you get off on the right foot.","title":"Overview"},{"location":"concepts/library-development/overview/#loading-libraries","text":"The libraries{} block within the Pipeline Configuration defines which libraries will be loaded for a particular Pipeline Run.","title":"Loading Libraries"},{"location":"concepts/library-development/overview/#learn-more","text":"Topic Description Library Structure Learn how files are organized within a library. Library Steps Learn how to add steps to a library. Library Resources Learn how to use static assets from within Library Steps Library Classes Learn how to define classes within a library. Parameterizing Libraries Learn how to make libraries configurable from the Pipeline Configuration Library Configuration File Learn how to validate library parameters using the library configuration file Library Sources Learn how to make a library discoverable for loading. Lifecycle Hooks Learn how to register steps for implicit invocation. Multi-Method Library Steps Learn how to define multiple methods per step. Step Aliasing Learn how to invoke the same step by multiple names.","title":"Learn More"},{"location":"concepts/library-development/parameterizing-libraries/","text":"Parameterizing Libraries \u00b6 One of the major benefits of organizing your pipeline code into libraries is the ability to reuse these libraries across different teams. To achieve this level of reusability, it's best to externalize hard coded values as parameters that can be set from the Pipeline Configuration repository. Pass Parameters Through the Pipeline Configuration \u00b6 When specifying a library to be loaded, users can pass arbitrary configurations to the library: libraries { example { [ 1 ] someField = \"my value\" [ 2 ] nested { [ 3 ] someOtherField = 11 [ 4 ] } } } The name of the library to be loaded A root level library configuration option A block name to pass nested configuration A nested library configuration Parameter Structure and Type \u00b6 Library parameters can take an arbitrary structure. All parameters can be at the root level or a nested structure can be created to group related configurations together. Library parameter values can be any serializable Groovy primitive. Typically, parameters are boolean, numeric, String, or array. Accessing Library Configurations Within Steps \u00b6 The Jenkins Templating Engine injects a config variable into each step. This config variable is a map whose keys are the library parameters that have been provided through the Pipeline Configuration. The config variable is only resolvable within a Library Step and only contains the configuration for the step's library. Note If you need to access the entire aggregated Pipeline Configuration, JTE injects a pipelineConfig variable that can be accessed anywhere. Validating Library Configurations \u00b6 The Pipeline Configuration doesn't inherently perform type checking or validation. Library developers can choose to provide a Library Configuration File at the root of the library's directory which will assist with library parameter validation.","title":"Parameterizing Libraries"},{"location":"concepts/library-development/parameterizing-libraries/#parameterizing-libraries","text":"One of the major benefits of organizing your pipeline code into libraries is the ability to reuse these libraries across different teams. To achieve this level of reusability, it's best to externalize hard coded values as parameters that can be set from the Pipeline Configuration repository.","title":"Parameterizing Libraries"},{"location":"concepts/library-development/parameterizing-libraries/#pass-parameters-through-the-pipeline-configuration","text":"When specifying a library to be loaded, users can pass arbitrary configurations to the library: libraries { example { [ 1 ] someField = \"my value\" [ 2 ] nested { [ 3 ] someOtherField = 11 [ 4 ] } } } The name of the library to be loaded A root level library configuration option A block name to pass nested configuration A nested library configuration","title":"Pass Parameters Through the Pipeline Configuration"},{"location":"concepts/library-development/parameterizing-libraries/#parameter-structure-and-type","text":"Library parameters can take an arbitrary structure. All parameters can be at the root level or a nested structure can be created to group related configurations together. Library parameter values can be any serializable Groovy primitive. Typically, parameters are boolean, numeric, String, or array.","title":"Parameter Structure and Type"},{"location":"concepts/library-development/parameterizing-libraries/#accessing-library-configurations-within-steps","text":"The Jenkins Templating Engine injects a config variable into each step. This config variable is a map whose keys are the library parameters that have been provided through the Pipeline Configuration. The config variable is only resolvable within a Library Step and only contains the configuration for the step's library. Note If you need to access the entire aggregated Pipeline Configuration, JTE injects a pipelineConfig variable that can be accessed anywhere.","title":"Accessing Library Configurations Within Steps"},{"location":"concepts/library-development/parameterizing-libraries/#validating-library-configurations","text":"The Pipeline Configuration doesn't inherently perform type checking or validation. Library developers can choose to provide a Library Configuration File at the root of the library's directory which will assist with library parameter validation.","title":"Validating Library Configurations"},{"location":"concepts/library-development/step-aliasing/","text":"Step Aliasing \u00b6 Step Aliasing allows library developers to cast the same step to one or more step names at runtime by using the @StepAlias annotation. By default, steps will assume the basename of the files that define them. i.e, a build.groovy step file will create a build step. Step Aliasing, allows you to change the name (or names) of the step that's going to be created. This annotation is automatically imported, just like lifecycle hooks . Static Step Aliases \u00b6 Static step aliases are static lists of strings to cast the step to at runtime. Single Static Alias \u00b6 @StepAlias can take a String parameter to change the name of the step at runtime. @StepAlias ( \"build\" ) [ 1 ] void call (){ println \"running as build!\" } generic.groovy will be invocable at runtime via build() Multiple Static Aliases \u00b6 @StepAlias can also accept an array of Strings to alias the step to multiple names. @StepAlias ([ \"build\" , \"unit_test\" ]) [ 1 ] void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test() Dynamic Step Aliases \u00b6 Sometimes, aliases should themselves be determined at runtime. This can be accomplished by providing a dynamic parameter that should be a Closure that returns a string or list of strings. For example, if a library called alias had a step called generic.groovy then an aliases library parameter could be created: libraries { alias { aliases = [ \"build\" , \"unit_test\" ] [ 1 ] } } defines a string or list of strings to alias the generic step to This aliases parameter can then be consumed within the dynamic step alias closure: @StepAlias ( dynamic = { return config . aliases }) [ 1 ] void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test() Keeping the Original Step \u00b6 By default, when @StepAlias is present in a step file, a step with the original name won't be created. This behavior can be overridden via the keepOriginal annotation parameter. @StepAlias ( value = \"build\" , keepOriginal = true ) [ 1 ] void call (){ println \"running as either build() or generic()\" } The keepOriginal parameter can be used if a step with the original step name should be created Note When passing multiple annotation parameters, the default static aliases parameter should be passed as value .","title":"Step Aliasing"},{"location":"concepts/library-development/step-aliasing/#step-aliasing","text":"Step Aliasing allows library developers to cast the same step to one or more step names at runtime by using the @StepAlias annotation. By default, steps will assume the basename of the files that define them. i.e, a build.groovy step file will create a build step. Step Aliasing, allows you to change the name (or names) of the step that's going to be created. This annotation is automatically imported, just like lifecycle hooks .","title":"Step Aliasing"},{"location":"concepts/library-development/step-aliasing/#static-step-aliases","text":"Static step aliases are static lists of strings to cast the step to at runtime.","title":"Static Step Aliases"},{"location":"concepts/library-development/step-aliasing/#single-static-alias","text":"@StepAlias can take a String parameter to change the name of the step at runtime. @StepAlias ( \"build\" ) [ 1 ] void call (){ println \"running as build!\" } generic.groovy will be invocable at runtime via build()","title":"Single Static Alias"},{"location":"concepts/library-development/step-aliasing/#multiple-static-aliases","text":"@StepAlias can also accept an array of Strings to alias the step to multiple names. @StepAlias ([ \"build\" , \"unit_test\" ]) [ 1 ] void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test()","title":"Multiple Static Aliases"},{"location":"concepts/library-development/step-aliasing/#dynamic-step-aliases","text":"Sometimes, aliases should themselves be determined at runtime. This can be accomplished by providing a dynamic parameter that should be a Closure that returns a string or list of strings. For example, if a library called alias had a step called generic.groovy then an aliases library parameter could be created: libraries { alias { aliases = [ \"build\" , \"unit_test\" ] [ 1 ] } } defines a string or list of strings to alias the generic step to This aliases parameter can then be consumed within the dynamic step alias closure: @StepAlias ( dynamic = { return config . aliases }) [ 1 ] void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test()","title":"Dynamic Step Aliases"},{"location":"concepts/library-development/step-aliasing/#keeping-the-original-step","text":"By default, when @StepAlias is present in a step file, a step with the original name won't be created. This behavior can be overridden via the keepOriginal annotation parameter. @StepAlias ( value = \"build\" , keepOriginal = true ) [ 1 ] void call (){ println \"running as either build() or generic()\" } The keepOriginal parameter can be used if a step with the original step name should be created Note When passing multiple annotation parameters, the default static aliases parameter should be passed as value .","title":"Keeping the Original Step"},{"location":"concepts/pipeline-configuration/configuration-dsl/","text":"Pipeline Configuration Syntax \u00b6 This page will cover the mechanics of JTE's Pipeline Configuration DSL. Motivation \u00b6 Originally, the JTE Pipeline Configuration was written in more standard structures like JSON or YAML. Structure Challenge JSON Too verbose to be comfortable writing. YAML Users would frequently make errors with YAML syntax that resulted in a different configuration than expected. In the end, a Groovy DSL provided the best of both words in terms of verbosity and forgiveness. Over time, the features made available through a custom DSL became useful. Data Structure Storage \u00b6 While not required to understand the DSL, it can accelerate your learning if you're familiar with LinkedHashMaps The Pipeline Configuration syntax is a nested builder language that relies on Blocks and Properties to build a LinkedHashMap representing the configuration. Property Setting \u00b6 Properties of the Pipeline Configuration are set using Groovy's Variable Assignment syntax. Pipeline Configuration DSL foo = \"bar\" Resulting pipelineConfig assert pipelineConfig == [ foo: \"bar\" ] Don't Declare Variables The DSL relies on setProperty(String propertyName, Object value) being executed to persist the Pipeline Configuration property values. Take the following example: Pipeline Configuration DSL x = \"x\" String y = \"y\" Resulting pipelineConfig assert pipelineConfig == [ x: \"x\" ] The y value is not persisted. Blocks \u00b6 The Pipeline Configuration DSL supports nested properties using Blocks. Pipeline Configuration a { x = 1 , y = 2 } Resulting pipelineConfig assert pipelineConfig == [ a: [ x: 1 y: 2 ] ] Empty Blocks and Unset Properties \u00b6 A special case is empty blocks and unset properties. Both situations result in an empty map being set in the Pipeline Configuration. Pipeline Config a { x = 1 y {} z } Resulting pipelineConfig assert pipelineConfig == [ a: [ x: 1 , y: [:], z: [:] ] ] Pipeline Governance Annotations \u00b6 To support Pipeline Governance , the Pipeline Configuration DSL uses special annotations to control which aspects of the configuration the next configuration in the Configuration Hierarchy is able to modify. These annotations are called @override and @merge and both can be placed on a block and property. Pipeline Configuration @merge a { x = 1 @override y = 2 } Learn More To learn more about how these annotations work, check out Merging Pipeline Configurations","title":"Pipeline Configuration Syntax"},{"location":"concepts/pipeline-configuration/configuration-dsl/#pipeline-configuration-syntax","text":"This page will cover the mechanics of JTE's Pipeline Configuration DSL.","title":"Pipeline Configuration Syntax"},{"location":"concepts/pipeline-configuration/configuration-dsl/#motivation","text":"Originally, the JTE Pipeline Configuration was written in more standard structures like JSON or YAML. Structure Challenge JSON Too verbose to be comfortable writing. YAML Users would frequently make errors with YAML syntax that resulted in a different configuration than expected. In the end, a Groovy DSL provided the best of both words in terms of verbosity and forgiveness. Over time, the features made available through a custom DSL became useful.","title":"Motivation"},{"location":"concepts/pipeline-configuration/configuration-dsl/#data-structure-storage","text":"While not required to understand the DSL, it can accelerate your learning if you're familiar with LinkedHashMaps The Pipeline Configuration syntax is a nested builder language that relies on Blocks and Properties to build a LinkedHashMap representing the configuration.","title":"Data Structure Storage"},{"location":"concepts/pipeline-configuration/configuration-dsl/#property-setting","text":"Properties of the Pipeline Configuration are set using Groovy's Variable Assignment syntax. Pipeline Configuration DSL foo = \"bar\" Resulting pipelineConfig assert pipelineConfig == [ foo: \"bar\" ] Don't Declare Variables The DSL relies on setProperty(String propertyName, Object value) being executed to persist the Pipeline Configuration property values. Take the following example: Pipeline Configuration DSL x = \"x\" String y = \"y\" Resulting pipelineConfig assert pipelineConfig == [ x: \"x\" ] The y value is not persisted.","title":"Property Setting"},{"location":"concepts/pipeline-configuration/configuration-dsl/#blocks","text":"The Pipeline Configuration DSL supports nested properties using Blocks. Pipeline Configuration a { x = 1 , y = 2 } Resulting pipelineConfig assert pipelineConfig == [ a: [ x: 1 y: 2 ] ]","title":"Blocks"},{"location":"concepts/pipeline-configuration/configuration-dsl/#empty-blocks-and-unset-properties","text":"A special case is empty blocks and unset properties. Both situations result in an empty map being set in the Pipeline Configuration. Pipeline Config a { x = 1 y {} z } Resulting pipelineConfig assert pipelineConfig == [ a: [ x: 1 , y: [:], z: [:] ] ]","title":"Empty Blocks and Unset Properties"},{"location":"concepts/pipeline-configuration/configuration-dsl/#pipeline-governance-annotations","text":"To support Pipeline Governance , the Pipeline Configuration DSL uses special annotations to control which aspects of the configuration the next configuration in the Configuration Hierarchy is able to modify. These annotations are called @override and @merge and both can be placed on a block and property. Pipeline Configuration @merge a { x = 1 @override y = 2 } Learn More To learn more about how these annotations work, check out Merging Pipeline Configurations","title":"Pipeline Governance Annotations"},{"location":"concepts/pipeline-configuration/merging-configs/","text":"Merging Configuration Files \u00b6 During Pipeline Initialization , JTE collects every Pipeline Configuration in the Configuration Hierarchy for the Pipeline Run. This Pipeline Configuration chain is then sequentially merged, starting with the top-most configuration and ending with the most granular. The following guidelines explain how two Pipeline Configurations are merged together. The First Pipeline Configuration \u00b6 The first Pipeline Configuration in the configuration chain can define any blocks and properties. Merging When A Parent Pipeline Configuration Is Present \u00b6 After the first Pipeline Configuration has been set, each subsequent Pipeline Configuration can define root-level blocks and properties but can't modify properties or blocks that were previously set unless explicitly permitted by the previous configuration. Permitting Modifications \u00b6 Pipeline Configurations must explicitly define which blocks and properties can be modified by the next configuration in the configuration chain. This is done through the @override and @merge annotations. @override \u00b6 The @override annotation is used to permit block-level changes or to permit specific properties to be changed. Setting @override on a block will allow the next configuration to change any property in the block. Setting @override on a property will allow the next configuration to change that property. @merge \u00b6 The @merge annotation is used at the block-level to allow the next configuration in the configuration chain to append properties to the block but not change inherited properties.","title":"Merging Configuration Files"},{"location":"concepts/pipeline-configuration/merging-configs/#merging-configuration-files","text":"During Pipeline Initialization , JTE collects every Pipeline Configuration in the Configuration Hierarchy for the Pipeline Run. This Pipeline Configuration chain is then sequentially merged, starting with the top-most configuration and ending with the most granular. The following guidelines explain how two Pipeline Configurations are merged together.","title":"Merging Configuration Files"},{"location":"concepts/pipeline-configuration/merging-configs/#the-first-pipeline-configuration","text":"The first Pipeline Configuration in the configuration chain can define any blocks and properties.","title":"The First Pipeline Configuration"},{"location":"concepts/pipeline-configuration/merging-configs/#merging-when-a-parent-pipeline-configuration-is-present","text":"After the first Pipeline Configuration has been set, each subsequent Pipeline Configuration can define root-level blocks and properties but can't modify properties or blocks that were previously set unless explicitly permitted by the previous configuration.","title":"Merging When A Parent Pipeline Configuration Is Present"},{"location":"concepts/pipeline-configuration/merging-configs/#permitting-modifications","text":"Pipeline Configurations must explicitly define which blocks and properties can be modified by the next configuration in the configuration chain. This is done through the @override and @merge annotations.","title":"Permitting Modifications"},{"location":"concepts/pipeline-configuration/merging-configs/#override","text":"The @override annotation is used to permit block-level changes or to permit specific properties to be changed. Setting @override on a block will allow the next configuration to change any property in the block. Setting @override on a property will allow the next configuration to change that property.","title":"@override"},{"location":"concepts/pipeline-configuration/merging-configs/#merge","text":"The @merge annotation is used at the block-level to allow the next configuration in the configuration chain to append properties to the block but not change inherited properties.","title":"@merge"},{"location":"concepts/pipeline-configuration/overview/","text":"Overview \u00b6 Pipeline Templates are generic, tool-agnostic workflows that utilize Pipeline Primitives to become concrete for specific teams. The Pipeline Configuration is what determines for a given Pipeline Run which Pipeline Template and which Pipeline Primitives should be used. Structure \u00b6 JTE's Pipeline Configuration uses a custom Domain-Specific Language (DSL) which after being parsed by JTE builds a Map . This DSL is a dynamic builder language. It doesn't validate that block names and fields align to the Pipeline Configuration Schema in any way. If a block or field is declared that's not in the schema, it will simply be ignored during Pipeline Initialization such that no Pipeline Primitives are created. The incorrect fields will still be accessible on the pipelineConfig autowired variable Script Security \u00b6 The Pipeline Configuration file is parsed by executing it within the same Groovy Sandbox that Jenkins pipelines use as well. Pipeline Configuration Location \u00b6 Configuration Hierarchy \u00b6 Pipeline Configurations can be stored in the Configuration Hierarchy on Governance Tiers . Merging Pipeline Configurations When more than one Pipeline Configuration is present for a given Pipeline Run, they're merged according to the rules outlined on Merging Configurations . Job-Level Pipeline Configurations \u00b6 Pipeline Configurations can be stored in a couple different locations depending on the Job Type. Job Type Pipeline Configuration Location Pipeline Job Either in the Jenkins UI or at the root of a remote source code repository as a file called pipeline_config.groovy Multi-Branch Project At the root of the repository in a file named pipeline_config.groovy in the branch job that was created as part of the Multi-Branch Project","title":"Overview"},{"location":"concepts/pipeline-configuration/overview/#overview","text":"Pipeline Templates are generic, tool-agnostic workflows that utilize Pipeline Primitives to become concrete for specific teams. The Pipeline Configuration is what determines for a given Pipeline Run which Pipeline Template and which Pipeline Primitives should be used.","title":"Overview"},{"location":"concepts/pipeline-configuration/overview/#structure","text":"JTE's Pipeline Configuration uses a custom Domain-Specific Language (DSL) which after being parsed by JTE builds a Map . This DSL is a dynamic builder language. It doesn't validate that block names and fields align to the Pipeline Configuration Schema in any way. If a block or field is declared that's not in the schema, it will simply be ignored during Pipeline Initialization such that no Pipeline Primitives are created. The incorrect fields will still be accessible on the pipelineConfig autowired variable","title":"Structure"},{"location":"concepts/pipeline-configuration/overview/#script-security","text":"The Pipeline Configuration file is parsed by executing it within the same Groovy Sandbox that Jenkins pipelines use as well.","title":"Script Security"},{"location":"concepts/pipeline-configuration/overview/#pipeline-configuration-location","text":"","title":"Pipeline Configuration Location"},{"location":"concepts/pipeline-configuration/overview/#configuration-hierarchy","text":"Pipeline Configurations can be stored in the Configuration Hierarchy on Governance Tiers . Merging Pipeline Configurations When more than one Pipeline Configuration is present for a given Pipeline Run, they're merged according to the rules outlined on Merging Configurations .","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-configuration/overview/#job-level-pipeline-configurations","text":"Pipeline Configurations can be stored in a couple different locations depending on the Job Type. Job Type Pipeline Configuration Location Pipeline Job Either in the Jenkins UI or at the root of a remote source code repository as a file called pipeline_config.groovy Multi-Branch Project At the root of the repository in a file named pipeline_config.groovy in the branch job that was created as part of the Multi-Branch Project","title":"Job-Level Pipeline Configurations"},{"location":"concepts/pipeline-governance/configuration-hierarchy/","text":"Configuration Hierarchy \u00b6 The Configuration Hierarchy is created by configuring these Governance Tiers on Folders 1 and in the Jenkins Global Configuration 2 . Pipelines using JTE inherit Pipeline Configuration , Pipeline Catalogs , and Library Sources from their parent Governance Tiers as determined by the hierarchy. Figure 1. Creating a Configuration Hierarchy The Folders Plugin allows users to define custom taxonomies. \u21a9 You can find the Global Configuration, if you have permission, by navigating to Manage Jenkins > Configure System \u21a9","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-governance/configuration-hierarchy/#configuration-hierarchy","text":"The Configuration Hierarchy is created by configuring these Governance Tiers on Folders 1 and in the Jenkins Global Configuration 2 . Pipelines using JTE inherit Pipeline Configuration , Pipeline Catalogs , and Library Sources from their parent Governance Tiers as determined by the hierarchy. Figure 1. Creating a Configuration Hierarchy The Folders Plugin allows users to define custom taxonomies. \u21a9 You can find the Global Configuration, if you have permission, by navigating to Manage Jenkins > Configure System \u21a9","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-governance/governance-tier/","text":"Governance Tier \u00b6 Governance Tiers are nodes in the Configuration Hierarchy that store the following: Governance Tier Data Description Pipeline Catalog A set of Pipeline Templates Pipeline Configuration A Pipeline Configuration that will be inherited Library Sources A list of Library Sources Reference: Governance Tier Structure To learn more about how to configure a Governance Tier, check out the Governance Tier Reference Page","title":"Governance Tier"},{"location":"concepts/pipeline-governance/governance-tier/#governance-tier","text":"Governance Tiers are nodes in the Configuration Hierarchy that store the following: Governance Tier Data Description Pipeline Catalog A set of Pipeline Templates Pipeline Configuration A Pipeline Configuration that will be inherited Library Sources A list of Library Sources Reference: Governance Tier Structure To learn more about how to configure a Governance Tier, check out the Governance Tier Reference Page","title":"Governance Tier"},{"location":"concepts/pipeline-governance/library-resolution/","text":"Library Resolution \u00b6 Each Governance Tier in the Configuration Hierarchy can store a list of Library Sources . This page explains the order in which JTE will try to resolve and load a Library if multiple Library Sources have the same library. Default Resolution Order \u00b6 JTE will search for Libraries within Library Sources starting with the Governance Tier most proximal to the Job in the taxonomy. The Library Sources will be queried for the library starting with the first Library Source in the list on the Governance Tier before proceeding to any subsequent Library Sources. If the Library Sources configured on the first Governance Tier don't have the Library being loaded, JTE will then check the parent Governance Tier. If the Library can't be found after searching every Governance Tier, the Pipeline Run will fail. Inverting Resolution Order \u00b6 To invert this resolution order, set jte.reverse_library_resolution to True .","title":"Library Resolution"},{"location":"concepts/pipeline-governance/library-resolution/#library-resolution","text":"Each Governance Tier in the Configuration Hierarchy can store a list of Library Sources . This page explains the order in which JTE will try to resolve and load a Library if multiple Library Sources have the same library.","title":"Library Resolution"},{"location":"concepts/pipeline-governance/library-resolution/#default-resolution-order","text":"JTE will search for Libraries within Library Sources starting with the Governance Tier most proximal to the Job in the taxonomy. The Library Sources will be queried for the library starting with the first Library Source in the list on the Governance Tier before proceeding to any subsequent Library Sources. If the Library Sources configured on the first Governance Tier don't have the Library being loaded, JTE will then check the parent Governance Tier. If the Library can't be found after searching every Governance Tier, the Pipeline Run will fail.","title":"Default Resolution Order"},{"location":"concepts/pipeline-governance/library-resolution/#inverting-resolution-order","text":"To invert this resolution order, set jte.reverse_library_resolution to True .","title":"Inverting Resolution Order"},{"location":"concepts/pipeline-governance/overview/","text":"Overview \u00b6 One of the Key Benefits of using JTE is the governance it can bring to software delivery. JTE achieves this governance by creating a Configuration Hierarchy using Jenkins global settings and Folder properties. The nodes of this hierarchy, called Governance Tiers , store Pipeline Configurations , a Pipeline Catalog , and Library Sources . Teams can create arbitrarily complex governance hierarchies simply by organizing jobs in Jenkins into the appropriate Folders. Learn More \u00b6 Page Description Configuration Hierarchy Learn how to set up hierarchical Pipeline Configurations Governance Tier Learn how to configure a node of the Configuration Hierarchy Pipeline Template Selection Learn how JTE determines which Pipeline Template to use for a given Pipeline Run Library Resolution Learn how JTE choose which library to load when there are multiple choices within the available Library Sources Governance Tier Learn how to configure a Governance Tier","title":"Overview"},{"location":"concepts/pipeline-governance/overview/#overview","text":"One of the Key Benefits of using JTE is the governance it can bring to software delivery. JTE achieves this governance by creating a Configuration Hierarchy using Jenkins global settings and Folder properties. The nodes of this hierarchy, called Governance Tiers , store Pipeline Configurations , a Pipeline Catalog , and Library Sources . Teams can create arbitrarily complex governance hierarchies simply by organizing jobs in Jenkins into the appropriate Folders.","title":"Overview"},{"location":"concepts/pipeline-governance/overview/#learn-more","text":"Page Description Configuration Hierarchy Learn how to set up hierarchical Pipeline Configurations Governance Tier Learn how to configure a node of the Configuration Hierarchy Pipeline Template Selection Learn how JTE determines which Pipeline Template to use for a given Pipeline Run Library Resolution Learn how JTE choose which library to load when there are multiple choices within the available Library Sources Governance Tier Learn how to configure a Governance Tier","title":"Learn More"},{"location":"concepts/pipeline-governance/pipeline-template-selection/","text":"Pipeline Template Selection \u00b6 Pipeline Template Selection is the name of the process that determines which Pipeline Template to use for a given Pipeline Run. Figure 1 visualizes this process in a flow chart. Figure 1. Pipeline Template Selection Flow Chart Job Type Matters \u00b6 Ad Hoc Pipeline Jobs \u00b6 JTE treats ad hoc Pipeline Jobs a little differently than Pipeline Jobs that have been created by a MultiBranch Project. For Pipeline Jobs, if a Pipeline Template has been configured, it will be used. If not, JTE will follow the flow described throughout the rest of this document. Rationale The rationale for using the configured template without falling back to the rest of the Pipeline Template Selection process is that if a user has permissions to create and configure their own Jenkins job, Pipeline Governance is already gone. MultiBranch Project Pipeline Jobs \u00b6 For MultiBranch Project Pipeline Jobs, if the source code repository has a Jenkinsfile at the root and jte.allow_scm_jenkinsfile is set to True , then the repository Jenkinsfile will be used as the Pipeline Template. Disabling Repository Jenkinsfiles It's important that when trying to enforce a certain set of Pipeline Templates are used that jte.allow_scm_jenkinsfile is set to False . Otherwise, developers will be able to write whatever Pipeline Template they want to. Named Pipeline Templates \u00b6 The next possibility is that the aggregated Pipeline Configuration has configured JTE to look for a Named Pipeline Template from the Pipeline Catalog . If this is the case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for the Named Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If the Named Pipeline Template can't be found, the Pipeline Run will fail. Finding the Default Pipeline Template \u00b6 Finally, if the job doesn't have a configured Pipeline Template and the Pipeline Configuration hasn't defined a Named Pipeline Template to use then JTE will search to find a Default Pipeline Template. In this case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for a Default Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If a Default Pipeline Template can't be found, the Pipeline Run will fail.","title":"Pipeline Template Selection"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#pipeline-template-selection","text":"Pipeline Template Selection is the name of the process that determines which Pipeline Template to use for a given Pipeline Run. Figure 1 visualizes this process in a flow chart. Figure 1. Pipeline Template Selection Flow Chart","title":"Pipeline Template Selection"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#job-type-matters","text":"","title":"Job Type Matters"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#ad-hoc-pipeline-jobs","text":"JTE treats ad hoc Pipeline Jobs a little differently than Pipeline Jobs that have been created by a MultiBranch Project. For Pipeline Jobs, if a Pipeline Template has been configured, it will be used. If not, JTE will follow the flow described throughout the rest of this document. Rationale The rationale for using the configured template without falling back to the rest of the Pipeline Template Selection process is that if a user has permissions to create and configure their own Jenkins job, Pipeline Governance is already gone.","title":"Ad Hoc Pipeline Jobs"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#multibranch-project-pipeline-jobs","text":"For MultiBranch Project Pipeline Jobs, if the source code repository has a Jenkinsfile at the root and jte.allow_scm_jenkinsfile is set to True , then the repository Jenkinsfile will be used as the Pipeline Template. Disabling Repository Jenkinsfiles It's important that when trying to enforce a certain set of Pipeline Templates are used that jte.allow_scm_jenkinsfile is set to False . Otherwise, developers will be able to write whatever Pipeline Template they want to.","title":"MultiBranch Project Pipeline Jobs"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#named-pipeline-templates","text":"The next possibility is that the aggregated Pipeline Configuration has configured JTE to look for a Named Pipeline Template from the Pipeline Catalog . If this is the case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for the Named Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If the Named Pipeline Template can't be found, the Pipeline Run will fail.","title":"Named Pipeline Templates"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#finding-the-default-pipeline-template","text":"Finally, if the job doesn't have a configured Pipeline Template and the Pipeline Configuration hasn't defined a Named Pipeline Template to use then JTE will search to find a Default Pipeline Template. In this case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for a Default Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If a Default Pipeline Template can't be found, the Pipeline Run will fail.","title":"Finding the Default Pipeline Template"},{"location":"concepts/pipeline-primitives/application-environments/","text":"Application Environments \u00b6 The Application Environment primitive allows users to encapsulate environmental context. Users can define custom fields from the Pipeline Configuration . Defining Application Environments \u00b6 The application_environments{} block is used to define Application Environments. Within the Application Environments block, environments are defined through nested keys. For example, the following code block would create dev and test variables, each referencing an Application Environment object. These variables can be resolved within the Pipeline Template or Library Steps. application_environments { dev test } Default Fields \u00b6 Application Environments can define the optional fields short_name and long_name . If not declared, these fields will default to the Application Environment key. For example: application_environments { dev test { short_name = \"t\" } staging { long_name = \"Staging\" } prod { short_name = \"p\" long_name = \"Production\" } } This block defines dev , test , and prod Application Environments. The following table outlines the values of short_name and long_name for each Application Environment. Application Environment Short Name Long Name dev \"dev\" \"dev\" test \"t\" \"test\" staging \"staging\" \"Staging\" prod \"p\" \"Production\" Determining Application Environment Order \u00b6 The order Application Environments are defined within the Pipeline Configuration are used to define previous and next properties. For example, defining the following Application Environments application_environments { dev test prod } would result in the following values for previous and next on each Application Environment: Application Environment previous next dev null test test dev prod prod test null Note These properties are automatically configured based upon the declaration order within the Pipeline Configuration. If you try to set the previous and next properties in the environment's definition an exception will be thrown. Custom Fields \u00b6 Application Environments accept custom fields. These custom fields can be used to capture characteristics about the Application Environment that should be used from the pipeline. Examples include AWS tags to use when querying infrastructure, kubernetes cluster API endpoints, IP addresses, etc. For example, if there were IP addresses that the pipeline needed to access during execution: application_environments { dev { ip_addresses = [ \"1.2.3.4\" , \"1.2.3.5\" ] } test prod { ip_addresses = [ \"1.2.3.6\" , \"1.2.3.7\" ] } } This would add an ip_addresses property to the dev and prod objects while test.ip_addresses would be null . Using Application Environments in Deployment Steps \u00b6 A common pattern is to use Application Environments in conjunction with steps that perform automated deployments. If a library were to contribute a deploy_to step that accepted an Application Environment as an input parameter, then a Pipeline Template could be created that leverages these variables. do_some_tests () deploy_to dev deploy_to prod A contrived example of a Library Step that follows this pattern is below. // within deploy_to.groovy void call ( app_env ){ // use the default long_name property to dynamically name the stage stage ( \"Deploy to ${app_env.long_name}\" ){ // iterate over the environment's ip addresses and print a statement app_env . ip_addresses . each { ip -> println \"publishing artifact to ${ip}\" } } } Note You may have noticed that the template didn't use parenthesis when invoking the deploy_to method: deploy_to dev . This has nothing to do with JTE. Groovy allows you to omit parentheses when passing parameters to a method.","title":"Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#application-environments","text":"The Application Environment primitive allows users to encapsulate environmental context. Users can define custom fields from the Pipeline Configuration .","title":"Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#defining-application-environments","text":"The application_environments{} block is used to define Application Environments. Within the Application Environments block, environments are defined through nested keys. For example, the following code block would create dev and test variables, each referencing an Application Environment object. These variables can be resolved within the Pipeline Template or Library Steps. application_environments { dev test }","title":"Defining Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#default-fields","text":"Application Environments can define the optional fields short_name and long_name . If not declared, these fields will default to the Application Environment key. For example: application_environments { dev test { short_name = \"t\" } staging { long_name = \"Staging\" } prod { short_name = \"p\" long_name = \"Production\" } } This block defines dev , test , and prod Application Environments. The following table outlines the values of short_name and long_name for each Application Environment. Application Environment Short Name Long Name dev \"dev\" \"dev\" test \"t\" \"test\" staging \"staging\" \"Staging\" prod \"p\" \"Production\"","title":"Default Fields"},{"location":"concepts/pipeline-primitives/application-environments/#determining-application-environment-order","text":"The order Application Environments are defined within the Pipeline Configuration are used to define previous and next properties. For example, defining the following Application Environments application_environments { dev test prod } would result in the following values for previous and next on each Application Environment: Application Environment previous next dev null test test dev prod prod test null Note These properties are automatically configured based upon the declaration order within the Pipeline Configuration. If you try to set the previous and next properties in the environment's definition an exception will be thrown.","title":"Determining Application Environment Order"},{"location":"concepts/pipeline-primitives/application-environments/#custom-fields","text":"Application Environments accept custom fields. These custom fields can be used to capture characteristics about the Application Environment that should be used from the pipeline. Examples include AWS tags to use when querying infrastructure, kubernetes cluster API endpoints, IP addresses, etc. For example, if there were IP addresses that the pipeline needed to access during execution: application_environments { dev { ip_addresses = [ \"1.2.3.4\" , \"1.2.3.5\" ] } test prod { ip_addresses = [ \"1.2.3.6\" , \"1.2.3.7\" ] } } This would add an ip_addresses property to the dev and prod objects while test.ip_addresses would be null .","title":"Custom Fields"},{"location":"concepts/pipeline-primitives/application-environments/#using-application-environments-in-deployment-steps","text":"A common pattern is to use Application Environments in conjunction with steps that perform automated deployments. If a library were to contribute a deploy_to step that accepted an Application Environment as an input parameter, then a Pipeline Template could be created that leverages these variables. do_some_tests () deploy_to dev deploy_to prod A contrived example of a Library Step that follows this pattern is below. // within deploy_to.groovy void call ( app_env ){ // use the default long_name property to dynamically name the stage stage ( \"Deploy to ${app_env.long_name}\" ){ // iterate over the environment's ip addresses and print a statement app_env . ip_addresses . each { ip -> println \"publishing artifact to ${ip}\" } } } Note You may have noticed that the template didn't use parenthesis when invoking the deploy_to method: deploy_to dev . This has nothing to do with JTE. Groovy allows you to omit parentheses when passing parameters to a method.","title":"Using Application Environments in Deployment Steps"},{"location":"concepts/pipeline-primitives/keywords/","text":"Keywords \u00b6 Keywords let users declare variables from the Pipeline Configuration that can be resolved from the Pipeline Template or Library Steps . Defining Keywords \u00b6 Keywords are defined via the keywords{} block in the Pipeline Configuration. For example, keywords { foo = \"bar\" } would then result in a foo variable with the value \"bar\" . Use Cases \u00b6 Global Variables \u00b6 Keywords can be used to define a globals variable accessible from the Pipeline Template and Library Steps. keywords { globals { one = 1 two = 2 } } Regular Expressions for Conditionals \u00b6 Keywords can be used to define regular expressions corresponding to common branch names for use from the Pipeline Template to keep the template easy to read. Pipeline Configuration keywords { main = ~ /^[mM]a(in|ster)$/ develop = ~ /^[Dd]evelop(ment|er|)$/ } Pipeline Template on_pull_request to: develop , { /* execute on a PR to branches matching the regular expression defined by the \"develop\" keyword */ } on_pull_request to: main , from: develop , { /* execute on a PR from a branch matching the regular expression defined by the \"develop\" keyword to a branch matching the regular expression defined by the \"main\" keyword */ } on_merge to: main , { /* execute when a PR is merged into a branch that matches the regular expression defined by the \"main\" keyword */ } Note The steps in this example ( on_pull_request and on_merge ) aren't a part of the Jenkins Templating Engine.","title":"Keywords"},{"location":"concepts/pipeline-primitives/keywords/#keywords","text":"Keywords let users declare variables from the Pipeline Configuration that can be resolved from the Pipeline Template or Library Steps .","title":"Keywords"},{"location":"concepts/pipeline-primitives/keywords/#defining-keywords","text":"Keywords are defined via the keywords{} block in the Pipeline Configuration. For example, keywords { foo = \"bar\" } would then result in a foo variable with the value \"bar\" .","title":"Defining Keywords"},{"location":"concepts/pipeline-primitives/keywords/#use-cases","text":"","title":"Use Cases"},{"location":"concepts/pipeline-primitives/keywords/#global-variables","text":"Keywords can be used to define a globals variable accessible from the Pipeline Template and Library Steps. keywords { globals { one = 1 two = 2 } }","title":"Global Variables"},{"location":"concepts/pipeline-primitives/keywords/#regular-expressions-for-conditionals","text":"Keywords can be used to define regular expressions corresponding to common branch names for use from the Pipeline Template to keep the template easy to read. Pipeline Configuration keywords { main = ~ /^[mM]a(in|ster)$/ develop = ~ /^[Dd]evelop(ment|er|)$/ } Pipeline Template on_pull_request to: develop , { /* execute on a PR to branches matching the regular expression defined by the \"develop\" keyword */ } on_pull_request to: main , from: develop , { /* execute on a PR from a branch matching the regular expression defined by the \"develop\" keyword to a branch matching the regular expression defined by the \"main\" keyword */ } on_merge to: main , { /* execute when a PR is merged into a branch that matches the regular expression defined by the \"main\" keyword */ } Note The steps in this example ( on_pull_request and on_merge ) aren't a part of the Jenkins Templating Engine.","title":"Regular Expressions for Conditionals"},{"location":"concepts/pipeline-primitives/overview/","text":"Overview \u00b6 Pipeline Primitives are objects that can be defined from the Pipeline Configuration and accessed from a Pipeline Template . Pipeline Primitives exist to make Pipeline Templates easier to write, easier to read, and easier to share across teams. Pipeline Primitive Types \u00b6 Primitive Type Description Steps Define a step of the pipeline, typically to be invoked from the Pipeline Template. Stages Group steps together to keep templates DRY. Application Environments Encapsulate environmental context Keywords Declare variables from the Pipeline Configuration for use in Pipeline Templates and steps","title":"Overview"},{"location":"concepts/pipeline-primitives/overview/#overview","text":"Pipeline Primitives are objects that can be defined from the Pipeline Configuration and accessed from a Pipeline Template . Pipeline Primitives exist to make Pipeline Templates easier to write, easier to read, and easier to share across teams.","title":"Overview"},{"location":"concepts/pipeline-primitives/overview/#pipeline-primitive-types","text":"Primitive Type Description Steps Define a step of the pipeline, typically to be invoked from the Pipeline Template. Stages Group steps together to keep templates DRY. Application Environments Encapsulate environmental context Keywords Declare variables from the Pipeline Configuration for use in Pipeline Templates and steps","title":"Pipeline Primitive Types"},{"location":"concepts/pipeline-primitives/primitive-namespace/","text":"Primitive Namespace \u00b6 The Pipeline Primitive Namespace is an Autowired Variable called jte that's accessible everywhere. It can be used to access all the loaded Pipeline Primitives for a given Pipeline Run. Accessing Libraries and Steps \u00b6 If libraries were loaded, the jte variable will have a libraries property that stores the library's steps. Invoking a Step Using The Primitive Namespace Pipeline Configuration libraries { npm // contributes a build() step } Pipeline Template jte . libraries . npm . build () Accessing Keywords \u00b6 If Keywords were defined, the jte variable will have a keywords property that stores the Keywords. Accessing Keywords Pipeline Configuration keywords { foo = \"bar\" } Pipeline Template assert jte . keywords . foo == \"bar\" Accessing Application Environments \u00b6 If Application Environments were defined, the jte variable will have an application_environments property that stores the Application Environments. Accessing Application Environments Pipeline Configuration application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Pipeline Template assert jte . application_environments . dev . ip == \"1.1.1.1\" assert jte . application_environments . prod . ip == \"2.2.2.2\" Accessing Stages \u00b6 If Stages were defined, the jte variable will have an stages property that stores the Stages. Accessing Application Environments Pipeline Configuration libraries { npm // contributes unit_test, build } stages { continuous_integration { unit_test build } } Pipeline Template jte . stages . continuous_integration ()","title":"Primitive Namespace"},{"location":"concepts/pipeline-primitives/primitive-namespace/#primitive-namespace","text":"The Pipeline Primitive Namespace is an Autowired Variable called jte that's accessible everywhere. It can be used to access all the loaded Pipeline Primitives for a given Pipeline Run.","title":"Primitive Namespace"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-libraries-and-steps","text":"If libraries were loaded, the jte variable will have a libraries property that stores the library's steps. Invoking a Step Using The Primitive Namespace Pipeline Configuration libraries { npm // contributes a build() step } Pipeline Template jte . libraries . npm . build ()","title":"Accessing Libraries and Steps"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-keywords","text":"If Keywords were defined, the jte variable will have a keywords property that stores the Keywords. Accessing Keywords Pipeline Configuration keywords { foo = \"bar\" } Pipeline Template assert jte . keywords . foo == \"bar\"","title":"Accessing Keywords"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-application-environments","text":"If Application Environments were defined, the jte variable will have an application_environments property that stores the Application Environments. Accessing Application Environments Pipeline Configuration application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Pipeline Template assert jte . application_environments . dev . ip == \"1.1.1.1\" assert jte . application_environments . prod . ip == \"2.2.2.2\"","title":"Accessing Application Environments"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-stages","text":"If Stages were defined, the jte variable will have an stages property that stores the Stages. Accessing Application Environments Pipeline Configuration libraries { npm // contributes unit_test, build } stages { continuous_integration { unit_test build } } Pipeline Template jte . stages . continuous_integration ()","title":"Accessing Stages"},{"location":"concepts/pipeline-primitives/stages/","text":"Stages \u00b6 Stages help keep Pipeline Templates DRY by grouping steps together for execution. Defining Stages \u00b6 Stages are defined through the stages{} block. Each subkey references a step to be executed. Stage Context \u00b6 The stageContext variable allows a step to determine if it's being executed as part of a stage. Property Description stageContext.name The name of the stage being executed. Is set to null when the step execution is outside of a stage. stageContext.args A map of named parameters passed to the stage. Is equal to an empty map when not within a stage execution or if no parameters were provided. stageContext Example \u00b6 Assume a library called demo is available within a configured Library Source . Pipeline Configuration stages { continuous_integration { unit_test } } libraries { demo } Pipeline Template continuous_integration param1: \"foo\" , param2: \"bar\" unit_test () unit_test.groovy // demo/steps/unit_test.groovy void call (){ println \"stage name = ${stepContext.name}\" println \"param1 = ${stageContext.args.param1}\" println \"param2 = ${stageContext.args.param2}\" } The console log from this pipeline would look similar to: ... stage name = continuous_integration param1 = foo param2 = bar ... stage name = null param1 = null param2 = null Use Cases \u00b6 Continuous Integration \u00b6 A common example would be to create a continuous integration stage to keep templates DRY. Pipeline Configuration ... stages { continuous_integration { unit_test static_code_analysis build scan_artifact } } Pipeline Template on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev } on_merge to: main , { deploy_to prod }","title":"Stages"},{"location":"concepts/pipeline-primitives/stages/#stages","text":"Stages help keep Pipeline Templates DRY by grouping steps together for execution.","title":"Stages"},{"location":"concepts/pipeline-primitives/stages/#defining-stages","text":"Stages are defined through the stages{} block. Each subkey references a step to be executed.","title":"Defining Stages"},{"location":"concepts/pipeline-primitives/stages/#stage-context","text":"The stageContext variable allows a step to determine if it's being executed as part of a stage. Property Description stageContext.name The name of the stage being executed. Is set to null when the step execution is outside of a stage. stageContext.args A map of named parameters passed to the stage. Is equal to an empty map when not within a stage execution or if no parameters were provided.","title":"Stage Context"},{"location":"concepts/pipeline-primitives/stages/#stagecontext-example","text":"Assume a library called demo is available within a configured Library Source . Pipeline Configuration stages { continuous_integration { unit_test } } libraries { demo } Pipeline Template continuous_integration param1: \"foo\" , param2: \"bar\" unit_test () unit_test.groovy // demo/steps/unit_test.groovy void call (){ println \"stage name = ${stepContext.name}\" println \"param1 = ${stageContext.args.param1}\" println \"param2 = ${stageContext.args.param2}\" } The console log from this pipeline would look similar to: ... stage name = continuous_integration param1 = foo param2 = bar ... stage name = null param1 = null param2 = null","title":"stageContext Example"},{"location":"concepts/pipeline-primitives/stages/#use-cases","text":"","title":"Use Cases"},{"location":"concepts/pipeline-primitives/stages/#continuous-integration","text":"A common example would be to create a continuous integration stage to keep templates DRY. Pipeline Configuration ... stages { continuous_integration { unit_test static_code_analysis build scan_artifact } } Pipeline Template on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev } on_merge to: main , { deploy_to prod }","title":"Continuous Integration"},{"location":"concepts/pipeline-primitives/steps/","text":"Steps \u00b6 Pipeline Templates represent generic software delivery workflows. Pipeline Templates use Steps to represent tasks in that workflow. Best Practice It is recommended that Steps are named generically. For example, rather than npm_build() the Step should be named build() . By naming Steps generically, multiple libraries can implement the same Step. This allows teams to share the same Pipeline Template by loading different libraries via the Pipeline Configuration . Placeholder Steps \u00b6 Users can create Placeholder Steps that do nothing and serve as a no-op 1 Step. The primary purpose of these Placeholder Steps is to avoid a NoSuchMethodError being thrown when the Pipeline Template attempts to invoke a Step that hasn't been contributed by a library. To define Placeholder Steps, use the template_methods{} block. Example: Defining Placeholder Steps In the following example, a Pipeline Template expects to invoke a unit_test() and a build() step. It's expected that users will declare in their Pipeline Configurations libraries that implement these steps. In case that's not true, the template_methods{} block has been configured to substitute Placeholder Steps to avoid an exception being thrown. Pipeline Template unit_test () build () Pipeline Configuration template_methods { unit_test build } Build Log [Pipeline] Start of Pipeline [JTE][Step - null/unit_test.call()] [Pipeline] echo Step unit_test is not implemented. [JTE][Step - null/build.call()] [Pipeline] echo Step build is not implemented. [Pipeline] End of Pipeline Finished: SUCCESS Library Steps \u00b6 Library Steps are contributed by libraries. Users define in the Pipeline Configuration which libraries to load, if any. Learn More Learn more about how to create libraries over in the Library Development section. No Operation : a command that does nothing. \u21a9","title":"Steps"},{"location":"concepts/pipeline-primitives/steps/#steps","text":"Pipeline Templates represent generic software delivery workflows. Pipeline Templates use Steps to represent tasks in that workflow. Best Practice It is recommended that Steps are named generically. For example, rather than npm_build() the Step should be named build() . By naming Steps generically, multiple libraries can implement the same Step. This allows teams to share the same Pipeline Template by loading different libraries via the Pipeline Configuration .","title":"Steps"},{"location":"concepts/pipeline-primitives/steps/#placeholder-steps","text":"Users can create Placeholder Steps that do nothing and serve as a no-op 1 Step. The primary purpose of these Placeholder Steps is to avoid a NoSuchMethodError being thrown when the Pipeline Template attempts to invoke a Step that hasn't been contributed by a library. To define Placeholder Steps, use the template_methods{} block. Example: Defining Placeholder Steps In the following example, a Pipeline Template expects to invoke a unit_test() and a build() step. It's expected that users will declare in their Pipeline Configurations libraries that implement these steps. In case that's not true, the template_methods{} block has been configured to substitute Placeholder Steps to avoid an exception being thrown. Pipeline Template unit_test () build () Pipeline Configuration template_methods { unit_test build } Build Log [Pipeline] Start of Pipeline [JTE][Step - null/unit_test.call()] [Pipeline] echo Step unit_test is not implemented. [JTE][Step - null/build.call()] [Pipeline] echo Step build is not implemented. [Pipeline] End of Pipeline Finished: SUCCESS","title":"Placeholder Steps"},{"location":"concepts/pipeline-primitives/steps/#library-steps","text":"Library Steps are contributed by libraries. Users define in the Pipeline Configuration which libraries to load, if any. Learn More Learn more about how to create libraries over in the Library Development section. No Operation : a command that does nothing. \u21a9","title":"Library Steps"},{"location":"concepts/pipeline-templates/declarative-syntax/","text":"Declarative Syntax Support \u00b6 JTE has supported writing Pipeline Templates in Declarative Syntax since version 2.0 . Some Background \u00b6 JTE hasn't always supported Declarative Syntax. With JTE, pipeline authors can create Pipeline Templates that look like a custom DSL. Take the following Pipeline Template and Pipeline Configuration for example: Pipeline Template on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev penetration_test () integration_test () performance_test () } on_merge to: main , { deploy_to prod } Pipeline Configuration libraries { git // supplies on_pull_request, on_merge docker // supplies build npm // supplies unit test sonarqube // supplies static_code_analysis helm // supplies deploy_to zap // supplies penetration_test cypress // supplies integration_test jmeter // supplies performance_test() } stages { continuous_integration { build unit_test static_code_analysis } } application_environments { dev prod } keywords { develop = ~ /^[Dd]ev(elop|elopment|eloper|)$/ main = ~ /^[Mm](ain|aster)$/ } Many users, however, would still prefer to write Pipeline Templates in Declarative Syntax. Motivation \u00b6 As it's a fully featured programming environment, Scripted Pipeline offers a tremendous amount of flexibility and extensibility to Jenkins users. The Groovy learning-curve isn\u2019t typically desirable for all members of a given team, so Declarative Pipeline was created to offer a simpler and more opinionated syntax for authoring Jenkins Pipeline 1 . Declarative Syntax offers a simpler and more opinionated way to write Jenkins pipelines. Users familiar with Declarative Syntax can get started using JTE. Pipeline Primitives , including Library Steps , can be resolved from a Pipeline Template written in Declarative Syntax. Step Resolution \u00b6 There is one minor behavioral difference between Pipeline Templates written in Scripted Pipeline Syntax vs Declarative Pipeline Syntax in regard to Step Resolution. When a Library Step is loaded that overwrites a Jenkins DSL step, such as sh , then in Scripted Pipeline Templates the Library Step will take precedence whereas in Declarative Pipeline Templates the original sh implementation will take precedence. The way to bypass this in Declarative Syntax to invoke the Library Step is to invoke it from a script block. Declarative Step Resolution Example Declarative Pipeline Syntax Assume a sh Library Step has been loaded. pipeline { agent any stages { stage ( \"Example\" ){ steps { sh \"some script\" [ 1 ] script { sh \"some script\" [ 2 ] } } } } } This sh call would invoke the original Jenkins DSL Pipeline Step This sh call, in the script{} block, would invoke the loaded JTE Library Step Taken from the Declarative Syntax documentation. \u21a9","title":"Declarative Syntax Support"},{"location":"concepts/pipeline-templates/declarative-syntax/#declarative-syntax-support","text":"JTE has supported writing Pipeline Templates in Declarative Syntax since version 2.0 .","title":"Declarative Syntax Support"},{"location":"concepts/pipeline-templates/declarative-syntax/#some-background","text":"JTE hasn't always supported Declarative Syntax. With JTE, pipeline authors can create Pipeline Templates that look like a custom DSL. Take the following Pipeline Template and Pipeline Configuration for example: Pipeline Template on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev penetration_test () integration_test () performance_test () } on_merge to: main , { deploy_to prod } Pipeline Configuration libraries { git // supplies on_pull_request, on_merge docker // supplies build npm // supplies unit test sonarqube // supplies static_code_analysis helm // supplies deploy_to zap // supplies penetration_test cypress // supplies integration_test jmeter // supplies performance_test() } stages { continuous_integration { build unit_test static_code_analysis } } application_environments { dev prod } keywords { develop = ~ /^[Dd]ev(elop|elopment|eloper|)$/ main = ~ /^[Mm](ain|aster)$/ } Many users, however, would still prefer to write Pipeline Templates in Declarative Syntax.","title":"Some Background"},{"location":"concepts/pipeline-templates/declarative-syntax/#motivation","text":"As it's a fully featured programming environment, Scripted Pipeline offers a tremendous amount of flexibility and extensibility to Jenkins users. The Groovy learning-curve isn\u2019t typically desirable for all members of a given team, so Declarative Pipeline was created to offer a simpler and more opinionated syntax for authoring Jenkins Pipeline 1 . Declarative Syntax offers a simpler and more opinionated way to write Jenkins pipelines. Users familiar with Declarative Syntax can get started using JTE. Pipeline Primitives , including Library Steps , can be resolved from a Pipeline Template written in Declarative Syntax.","title":"Motivation"},{"location":"concepts/pipeline-templates/declarative-syntax/#step-resolution","text":"There is one minor behavioral difference between Pipeline Templates written in Scripted Pipeline Syntax vs Declarative Pipeline Syntax in regard to Step Resolution. When a Library Step is loaded that overwrites a Jenkins DSL step, such as sh , then in Scripted Pipeline Templates the Library Step will take precedence whereas in Declarative Pipeline Templates the original sh implementation will take precedence. The way to bypass this in Declarative Syntax to invoke the Library Step is to invoke it from a script block. Declarative Step Resolution Example Declarative Pipeline Syntax Assume a sh Library Step has been loaded. pipeline { agent any stages { stage ( \"Example\" ){ steps { sh \"some script\" [ 1 ] script { sh \"some script\" [ 2 ] } } } } } This sh call would invoke the original Jenkins DSL Pipeline Step This sh call, in the script{} block, would invoke the loaded JTE Library Step Taken from the Declarative Syntax documentation. \u21a9","title":"Step Resolution"},{"location":"concepts/pipeline-templates/overview/","text":"Overview \u00b6 In JTE, Pipeline Templates are used to define tool-agnostic workflows that can be shared across teams. Pipeline Templates make use of Pipeline Primitives to become reusable. Just A Jenkinsfile \u00b6 A Pipeline Template is executed exactly like a Jenkinsfile . In fact, there's almost no functional difference between a Jenkinsfile and a Pipeline Template in JTE. Regular Jenkins DSL pipeline steps like node , sh , and echo will all work as expected from a Pipeline Template. What's different, though, is what happens before the template is executed. During Pipeline Initialization , Pipeline Primitives are created and made available to the Pipeline Template. Defining Workflows, not Tech Stacks \u00b6 While creating JTE, it was envisioned that a Pipeline Template represents a workflow - not a pipeline for a specific tech stack. Be careful of the common pitfall of creating an npm template for all your NPM applications and a java template for all your Java applications. If you find yourself doing this - compare those templates and see if there would be a way to make converge with more general step names in a common workflow. One example of having multiple workflows would be if there were two branching strategies used throughout the organization or if JTE was being used for infrastructure pipelines as well as application pipelines. Do whatever works for you At the end of the day, JTE's goal is to make pipeline development easier at scale. Do whatever works best for your organization. Creating a contract between the pipeline and teams \u00b6 One way to think of a Pipeline Template is that it creates an \"API contract\" or interface between the pipeline and development teams. The Pipeline Configuration is what \"hydrates\" the template to make it concrete by declaring which Pipeline Primitives should be loaded. Learn More \u00b6 Page Description Pipeline Catalog Learn about how to build a catalog of Pipeline Templates teams can choose from Declarative Syntax Support Learn how to write templates using Jenkins Declarative Syntax","title":"Overview"},{"location":"concepts/pipeline-templates/overview/#overview","text":"In JTE, Pipeline Templates are used to define tool-agnostic workflows that can be shared across teams. Pipeline Templates make use of Pipeline Primitives to become reusable.","title":"Overview"},{"location":"concepts/pipeline-templates/overview/#just-a-jenkinsfile","text":"A Pipeline Template is executed exactly like a Jenkinsfile . In fact, there's almost no functional difference between a Jenkinsfile and a Pipeline Template in JTE. Regular Jenkins DSL pipeline steps like node , sh , and echo will all work as expected from a Pipeline Template. What's different, though, is what happens before the template is executed. During Pipeline Initialization , Pipeline Primitives are created and made available to the Pipeline Template.","title":"Just A Jenkinsfile"},{"location":"concepts/pipeline-templates/overview/#defining-workflows-not-tech-stacks","text":"While creating JTE, it was envisioned that a Pipeline Template represents a workflow - not a pipeline for a specific tech stack. Be careful of the common pitfall of creating an npm template for all your NPM applications and a java template for all your Java applications. If you find yourself doing this - compare those templates and see if there would be a way to make converge with more general step names in a common workflow. One example of having multiple workflows would be if there were two branching strategies used throughout the organization or if JTE was being used for infrastructure pipelines as well as application pipelines. Do whatever works for you At the end of the day, JTE's goal is to make pipeline development easier at scale. Do whatever works best for your organization.","title":"Defining Workflows, not Tech Stacks"},{"location":"concepts/pipeline-templates/overview/#creating-a-contract-between-the-pipeline-and-teams","text":"One way to think of a Pipeline Template is that it creates an \"API contract\" or interface between the pipeline and development teams. The Pipeline Configuration is what \"hydrates\" the template to make it concrete by declaring which Pipeline Primitives should be loaded.","title":"Creating a contract between the pipeline and teams"},{"location":"concepts/pipeline-templates/overview/#learn-more","text":"Page Description Pipeline Catalog Learn about how to build a catalog of Pipeline Templates teams can choose from Declarative Syntax Support Learn how to write templates using Jenkins Declarative Syntax","title":"Learn More"},{"location":"concepts/pipeline-templates/pipeline-catalog/","text":"Pipeline Catalog \u00b6 It's unlikely, especially when first starting out, that all your organization's pipelines will map to a single Pipeline Template . JTE supports having multiple Pipeline Templates that teams can choose from. These templates are organized into Pipeline Catalogs configured on Governance Tiers . Default Pipeline Template \u00b6 Pipeline Catalogs can have a default Pipeline Template . Named Pipeline Templates \u00b6 Additional Pipeline Templates are called Named Pipeline Templates . Learn More For more information, you can learn where templates are organized and how to create multiple templates over on the Governance Tier Reference Page page. You can learn more about how JTE chooses the Pipeline Template for a run over at Pipeline Template Selection .","title":"Pipeline Catalog"},{"location":"concepts/pipeline-templates/pipeline-catalog/#pipeline-catalog","text":"It's unlikely, especially when first starting out, that all your organization's pipelines will map to a single Pipeline Template . JTE supports having multiple Pipeline Templates that teams can choose from. These templates are organized into Pipeline Catalogs configured on Governance Tiers .","title":"Pipeline Catalog"},{"location":"concepts/pipeline-templates/pipeline-catalog/#default-pipeline-template","text":"Pipeline Catalogs can have a default Pipeline Template .","title":"Default Pipeline Template"},{"location":"concepts/pipeline-templates/pipeline-catalog/#named-pipeline-templates","text":"Additional Pipeline Templates are called Named Pipeline Templates . Learn More For more information, you can learn where templates are organized and how to create multiple templates over on the Governance Tier Reference Page page. You can learn more about how JTE chooses the Pipeline Template for a run over at Pipeline Template Selection .","title":"Named Pipeline Templates"},{"location":"contributing/fork-based/","text":"Fork-Based Contribution Model \u00b6 Regardless of whether you're contributing to the docs or the code, JTE follows a fork-based contribution model. Also called a Fork and Pull Model , it works through contributors Forking the main JTE repository . Contributors work on feature branches in their own forks and, when ready, submit a Pull Request back to the main JTE repository.","title":"Fork-Based Contribution Model"},{"location":"contributing/fork-based/#fork-based-contribution-model","text":"Regardless of whether you're contributing to the docs or the code, JTE follows a fork-based contribution model. Also called a Fork and Pull Model , it works through contributors Forking the main JTE repository . Contributors work on feature branches in their own forks and, when ready, submit a Pull Request back to the main JTE repository.","title":"Fork-Based Contribution Model"},{"location":"contributing/overview/","text":"Overview \u00b6 The Contributing section will help you get started with JTE development. The contributing docs have been broken into two sections: Section Description Documentation How to contribute to these docs Developer Docs How to get started developing JTE","title":"Overview"},{"location":"contributing/overview/#overview","text":"The Contributing section will help you get started with JTE development. The contributing docs have been broken into two sections: Section Description Documentation How to contribute to these docs Developer Docs How to get started developing JTE","title":"Overview"},{"location":"contributing/developer/building/","text":"Building the Plugin \u00b6 To build the JPI, run: just jpi Once built, the JPI will be located at build/libs/templating-engine.jpi","title":"Building the Plugin"},{"location":"contributing/developer/building/#building-the-plugin","text":"To build the JPI, run: just jpi Once built, the JPI will be located at build/libs/templating-engine.jpi","title":"Building the Plugin"},{"location":"contributing/developer/linting/","text":"Linting \u00b6 This project uses Spotless and CodeNarc to perform linting. The CodeNarc rule sets for src/main and src/test can be found in config/codenarc/rules.groovy and config/codenarc/rulesTest.groovy , respectively. To execute linting, run: just lint-code Once executed, the reports can be found at build/reports/codenarc/main.html and build/reports/codenarc/test.html .","title":"Linting"},{"location":"contributing/developer/linting/#linting","text":"This project uses Spotless and CodeNarc to perform linting. The CodeNarc rule sets for src/main and src/test can be found in config/codenarc/rules.groovy and config/codenarc/rulesTest.groovy , respectively. To execute linting, run: just lint-code Once executed, the reports can be found at build/reports/codenarc/main.html and build/reports/codenarc/test.html .","title":"Linting"},{"location":"contributing/developer/local-jenkins/","text":"Running A Local Jenkins \u00b6 It's often helpful to run Jenkins in a container locally to test various scenarios with JTE during development. just run With the default settings, this will expose jenkins on http://localhost:8080 Change the container name \u00b6 just --set container someName run Change the port forwarding target \u00b6 just --set port 9000 run Pass arbitrary flags to the container \u00b6 Parameters passed to just run are sent as flags to the docker run command. just run -e SOMEVAR = \"some var\" Mounting local libraries for testing \u00b6 Local directories can be configured as Git SCM Library Sources even if they don't have a remote repository. For example, if ~/local-libraries is a directory containing a local git repository then to mount it to the container you would run: just run -v ~/local-libraries:/local-libraries You could then configure a Library Source using the file protocol to specify the repository location at file:///local-libraries Tip When using this technique, changes to the libraries must be committed to be found. In a separate terminal, run: just watch ~/local-libraries to automatically commit changes to the libraries.","title":"Running A Local Jenkins"},{"location":"contributing/developer/local-jenkins/#running-a-local-jenkins","text":"It's often helpful to run Jenkins in a container locally to test various scenarios with JTE during development. just run With the default settings, this will expose jenkins on http://localhost:8080","title":"Running A Local Jenkins"},{"location":"contributing/developer/local-jenkins/#change-the-container-name","text":"just --set container someName run","title":"Change the container name"},{"location":"contributing/developer/local-jenkins/#change-the-port-forwarding-target","text":"just --set port 9000 run","title":"Change the port forwarding target"},{"location":"contributing/developer/local-jenkins/#pass-arbitrary-flags-to-the-container","text":"Parameters passed to just run are sent as flags to the docker run command. just run -e SOMEVAR = \"some var\"","title":"Pass arbitrary flags to the container"},{"location":"contributing/developer/local-jenkins/#mounting-local-libraries-for-testing","text":"Local directories can be configured as Git SCM Library Sources even if they don't have a remote repository. For example, if ~/local-libraries is a directory containing a local git repository then to mount it to the container you would run: just run -v ~/local-libraries:/local-libraries You could then configure a Library Source using the file protocol to specify the repository location at file:///local-libraries Tip When using this technique, changes to the libraries must be committed to be found. In a separate terminal, run: just watch ~/local-libraries to automatically commit changes to the libraries.","title":"Mounting local libraries for testing"},{"location":"contributing/developer/overview/","text":"Overview \u00b6 Local Environment \u00b6 Tool Purpose Gradle Used to run unit tests, package the JPI, and publish the plugin Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview Learn More \u00b6 Topic Description Running Tests Learn how to run JTE's test suite Building the Plugin Learn how to build the JTE JPI Linting Learn about JTE's code linting Running A Local Jenkins Learn how to run a local Jenkins for manual integration testing Publishing A Release Learn how to publish a JTE release","title":"Overview"},{"location":"contributing/developer/overview/#overview","text":"","title":"Overview"},{"location":"contributing/developer/overview/#local-environment","text":"Tool Purpose Gradle Used to run unit tests, package the JPI, and publish the plugin Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview","title":"Local Environment"},{"location":"contributing/developer/overview/#learn-more","text":"Topic Description Running Tests Learn how to run JTE's test suite Building the Plugin Learn how to build the JTE JPI Linting Learn about JTE's code linting Running A Local Jenkins Learn how to run a local Jenkins for manual integration testing Publishing A Release Learn how to publish a JTE release","title":"Learn More"},{"location":"contributing/developer/releasing/","text":"Publishing A Release \u00b6 If you have the permission , you can cut a new release of JTE by running just release <versionNumber> . For example: just release 2 .0.4 This will: create a release/2.0.4 branch update the version in the build.gradle update the version in the docs/antora.yml push those changes create a 2.0.4 tag publish the JPI Don't forget to go to the Release Page to officially release JTE with the current change log based off the most recent tag. Release Permissions Permissions are managed here . You'll need sign-off from one of the existing maintainers to be added.","title":"Publishing A Release"},{"location":"contributing/developer/releasing/#publishing-a-release","text":"If you have the permission , you can cut a new release of JTE by running just release <versionNumber> . For example: just release 2 .0.4 This will: create a release/2.0.4 branch update the version in the build.gradle update the version in the docs/antora.yml push those changes create a 2.0.4 tag publish the JPI Don't forget to go to the Release Page to officially release JTE with the current change log based off the most recent tag. Release Permissions Permissions are managed here . You'll need sign-off from one of the existing maintainers to be added.","title":"Publishing A Release"},{"location":"contributing/developer/running-tests/","text":"Running Tests \u00b6 Unit tests for JTE are written using Spock . To run all the tests, run: just test The gradle test report is published to build/reports/tests/test/index.html Execute tests for a specific class \u00b6 To run tests for a specific Class, StepWrapperSpec for example, run: just test '*.StepWrapperSpec' Code Coverage \u00b6 By default, JaCoCo is enabled when running test. Once executed, the JaCoCo coverage report can be found at: build/reports/jacoco/test/html/index.html To disable this, run: just --set coverage false test","title":"Running Tests"},{"location":"contributing/developer/running-tests/#running-tests","text":"Unit tests for JTE are written using Spock . To run all the tests, run: just test The gradle test report is published to build/reports/tests/test/index.html","title":"Running Tests"},{"location":"contributing/developer/running-tests/#execute-tests-for-a-specific-class","text":"To run tests for a specific Class, StepWrapperSpec for example, run: just test '*.StepWrapperSpec'","title":"Execute tests for a specific class"},{"location":"contributing/developer/running-tests/#code-coverage","text":"By default, JaCoCo is enabled when running test. Once executed, the JaCoCo coverage report can be found at: build/reports/jacoco/test/html/index.html To disable this, run: just --set coverage false test","title":"Code Coverage"},{"location":"contributing/docs/acronyms/","text":"On Acronyms \u00b6 Acronyms have a place in documentation but please follow the guidance provided by the Microsoft Writing Style Guide . If a new acronym is introduced, update the glossary at docs/glossary.md . Updating the glossary will provide hover-over expansion of the acronym. Glossary in Action Check out what happens when using acronyms from the glossary: DSL, JTE, SCM, IDE","title":"On Acronyms"},{"location":"contributing/docs/acronyms/#on-acronyms","text":"Acronyms have a place in documentation but please follow the guidance provided by the Microsoft Writing Style Guide . If a new acronym is introduced, update the glossary at docs/glossary.md . Updating the glossary will provide hover-over expansion of the acronym. Glossary in Action Check out what happens when using acronyms from the glossary: DSL, JTE, SCM, IDE","title":"On Acronyms"},{"location":"contributing/docs/add-or-remove-pages/","text":"Add or Remove Pages \u00b6 The page tree for this docs site is maintained in the mkdocs.yml file at the root of the repository. The structure nav section of the file defines how pages are organized. Important When adding or removing pages from the docs site, make sure to update mkdocs.yml to update the page tree.","title":"Add or Remove Pages"},{"location":"contributing/docs/add-or-remove-pages/#add-or-remove-pages","text":"The page tree for this docs site is maintained in the mkdocs.yml file at the root of the repository. The structure nav section of the file defines how pages are organized. Important When adding or removing pages from the docs site, make sure to update mkdocs.yml to update the page tree.","title":"Add or Remove Pages"},{"location":"contributing/docs/documentation-structure/","text":"Documentation Structure \u00b6 The JTE documentation is organized into the following sections: Concepts , Reference , Tutorials , & How-To Guides . Section Overview \u00b6 Oriented To Must Form Concepts understanding explain an article Reference information describe specifics no-fluff specifications Tutorials learning help new users get started a hands-on lesson How-To Guides a goal solve a specific problem a step-by-step walkthrough Note Huge shout out to Divio for formalizing this approach here . The previous table comes from the introduction to this documentation system. Finding What You Need \u00b6 The MkDocs config file ( mkdocs.yml ) at the root of the repository defines the page tree for the entire docs site. The following table outlines where the categories of documentation are stored. Section Location Concepts docs/concepts Reference docs/reference Tutorials docs/tutorials How-To Guides docs/how-to Contributing docs/contributing","title":"Documentation Structure"},{"location":"contributing/docs/documentation-structure/#documentation-structure","text":"The JTE documentation is organized into the following sections: Concepts , Reference , Tutorials , & How-To Guides .","title":"Documentation Structure"},{"location":"contributing/docs/documentation-structure/#section-overview","text":"Oriented To Must Form Concepts understanding explain an article Reference information describe specifics no-fluff specifications Tutorials learning help new users get started a hands-on lesson How-To Guides a goal solve a specific problem a step-by-step walkthrough Note Huge shout out to Divio for formalizing this approach here . The previous table comes from the introduction to this documentation system.","title":"Section Overview"},{"location":"contributing/docs/documentation-structure/#finding-what-you-need","text":"The MkDocs config file ( mkdocs.yml ) at the root of the repository defines the page tree for the entire docs site. The following table outlines where the categories of documentation are stored. Section Location Concepts docs/concepts Reference docs/reference Tutorials docs/tutorials How-To Guides docs/how-to Contributing docs/contributing","title":"Finding What You Need"},{"location":"contributing/docs/getting-started/","text":"Getting Started \u00b6 Presumably you're here because you want to help by updating the JTE documentation, so thank you! Tools \u00b6 These docs are written in Markdown 1 and compiled using MkDocs with the Material for MkDocs theme. Development activities take place within containers and are orchestrated using Just . Tool Description Just a task runner similar to Make with a simpler syntax Docker runtime environments are encapsulated in container images MkDocs Documentation framework Material for MkDocs Documentation styling Mike Documentation versioning Markdownlint Markdown Linter Vale Prose Linter Visual Studio Code Recommended IDE for docs development Required Prerequisites You can get by with only Just and Docker. Local installations of the other tools may prove useful for development but aren't required. Learn More \u00b6 Topic Description Documentation Structure Learn how JTE's docs are organized Local Development Learn how to make changes to the docs locally Markdownlint Learn more about the Markdown linter Prose Linting with Vale Learn more about the Prose linter Add or Remove Pages Learn how to update the page tree Markdown Cheatsheet Learn more about Markdown Syntax for this site Markdown \u21a9","title":"Getting Started"},{"location":"contributing/docs/getting-started/#getting-started","text":"Presumably you're here because you want to help by updating the JTE documentation, so thank you!","title":"Getting Started"},{"location":"contributing/docs/getting-started/#tools","text":"These docs are written in Markdown 1 and compiled using MkDocs with the Material for MkDocs theme. Development activities take place within containers and are orchestrated using Just . Tool Description Just a task runner similar to Make with a simpler syntax Docker runtime environments are encapsulated in container images MkDocs Documentation framework Material for MkDocs Documentation styling Mike Documentation versioning Markdownlint Markdown Linter Vale Prose Linter Visual Studio Code Recommended IDE for docs development Required Prerequisites You can get by with only Just and Docker. Local installations of the other tools may prove useful for development but aren't required.","title":"Tools"},{"location":"contributing/docs/getting-started/#learn-more","text":"Topic Description Documentation Structure Learn how JTE's docs are organized Local Development Learn how to make changes to the docs locally Markdownlint Learn more about the Markdown linter Prose Linting with Vale Learn more about the Prose linter Add or Remove Pages Learn how to update the page tree Markdown Cheatsheet Learn more about Markdown Syntax for this site Markdown \u21a9","title":"Learn More"},{"location":"contributing/docs/local-development/","text":"Local Development \u00b6 Local Docs Server \u00b6 MkDocs supports a local development server so that changes can be viewed in the browser in real-time as they're made. To see changes in real-time, run just serve . After a few seconds, a local version of the docs will be hosted at http://localhost:8000 . Prerequisites Check out the Prerequisites to make sure you've got the required tools installed. Integrated Development Environment Integration \u00b6 Visual Studio Code is the recommended IDE for updating the docs as it has extensions for both linting tools used: markdownlint Vale Local Tool Installation To use the VS Code extensions for markdownlint and vale you'll have to install those tools locally.","title":"Local Development"},{"location":"contributing/docs/local-development/#local-development","text":"","title":"Local Development"},{"location":"contributing/docs/local-development/#local-docs-server","text":"MkDocs supports a local development server so that changes can be viewed in the browser in real-time as they're made. To see changes in real-time, run just serve . After a few seconds, a local version of the docs will be hosted at http://localhost:8000 . Prerequisites Check out the Prerequisites to make sure you've got the required tools installed.","title":"Local Docs Server"},{"location":"contributing/docs/local-development/#integrated-development-environment-integration","text":"Visual Studio Code is the recommended IDE for updating the docs as it has extensions for both linting tools used: markdownlint Vale Local Tool Installation To use the VS Code extensions for markdownlint and vale you'll have to install those tools locally.","title":"Integrated Development Environment Integration"},{"location":"contributing/docs/markdown-cheatsheet/","text":"Markdown Cheatsheet \u00b6 Headers \u00b6 # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Emphasis \u00b6 Style Markdown Italics *Italics* Bold **Bold** Bold and Italics **_Bold and Italics_** Links \u00b6 There are 3 primary ways embed a hyperlink. Markdown 1. you can use an [ inline link ]( https://google.com ) 2. you can use a [ link by reference ][ 1 ] 3. you can use the [link text itself] as the reference [ 1 ]: https://google.com [ link text itself ]: https://google.com Rendered you can use an inline link you can use a link by reference you can use the link text itself as the reference Tables \u00b6 Tip Tables are kind of a pain in markdown. This tool can help to generate your markdown tables for you. The Markdown Table Prettifier extension for VS Code is also pretty great. Markdown | column 1 | column 2 | column 3 | | ---------- | :---------: | -----------: | | column 1 | column 2 | column 3 is | | is left | is | is right | | aligned | centered | aligned | Rendered column 1 column 2 column 3 column 1 column 2 column 3 is is left is is right aligned centered aligned Inline Code Snippets \u00b6 Markdown Inline `code` snippets use `backticks` around them Rendered Inline code snippets use backticks around them Code Blocks \u00b6 code blocks use three backticks and the language name for syntax highlighting: Markdown ```groovy def s = [ 1 , 2 , 3 ] s . each { item -> println item } ``` Rendered def s = [ 1 , 2 , 3 ] s . each { item -> println item } Admonitions \u00b6 Squidfunk covers this on the Admonitions page of the Material for MkDocs docs site. Please use consistent admonitions based upon the type of content being added. Admonition Type Description example Examples of what's being discussed tip A recommendation from the maintainers danger Call outs for common gotchas info Redirect users to more information Emojis \u00b6 To embed an emoji, simply surround the emoji name with two colons: :emoji-name: . A list of the available emojis can be found at emojipedia . Content Tabs \u00b6 Content Tabs have been used throughout this page. === \"Tab Title A\" some markdown content === \"Tab Title B\" some other markdown content Tab Title A some markdown content Tab Title B some other markdown content Footnotes \u00b6 Markdown Footnotes[^1] are supported. [ ^1 ]: some footnote information Rendered Footnotes 1 are supported. here's some footnote text \u21a9","title":"Markdown Cheatsheet"},{"location":"contributing/docs/markdown-cheatsheet/#markdown-cheatsheet","text":"","title":"Markdown Cheatsheet"},{"location":"contributing/docs/markdown-cheatsheet/#headers","text":"# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6","title":"Headers"},{"location":"contributing/docs/markdown-cheatsheet/#emphasis","text":"Style Markdown Italics *Italics* Bold **Bold** Bold and Italics **_Bold and Italics_**","title":"Emphasis"},{"location":"contributing/docs/markdown-cheatsheet/#links","text":"There are 3 primary ways embed a hyperlink. Markdown 1. you can use an [ inline link ]( https://google.com ) 2. you can use a [ link by reference ][ 1 ] 3. you can use the [link text itself] as the reference [ 1 ]: https://google.com [ link text itself ]: https://google.com Rendered you can use an inline link you can use a link by reference you can use the link text itself as the reference","title":"Links"},{"location":"contributing/docs/markdown-cheatsheet/#tables","text":"Tip Tables are kind of a pain in markdown. This tool can help to generate your markdown tables for you. The Markdown Table Prettifier extension for VS Code is also pretty great. Markdown | column 1 | column 2 | column 3 | | ---------- | :---------: | -----------: | | column 1 | column 2 | column 3 is | | is left | is | is right | | aligned | centered | aligned | Rendered column 1 column 2 column 3 column 1 column 2 column 3 is is left is is right aligned centered aligned","title":"Tables"},{"location":"contributing/docs/markdown-cheatsheet/#inline-code-snippets","text":"Markdown Inline `code` snippets use `backticks` around them Rendered Inline code snippets use backticks around them","title":"Inline Code Snippets"},{"location":"contributing/docs/markdown-cheatsheet/#code-blocks","text":"code blocks use three backticks and the language name for syntax highlighting: Markdown ```groovy def s = [ 1 , 2 , 3 ] s . each { item -> println item } ``` Rendered def s = [ 1 , 2 , 3 ] s . each { item -> println item }","title":"Code Blocks"},{"location":"contributing/docs/markdown-cheatsheet/#admonitions","text":"Squidfunk covers this on the Admonitions page of the Material for MkDocs docs site. Please use consistent admonitions based upon the type of content being added. Admonition Type Description example Examples of what's being discussed tip A recommendation from the maintainers danger Call outs for common gotchas info Redirect users to more information","title":"Admonitions"},{"location":"contributing/docs/markdown-cheatsheet/#emojis","text":"To embed an emoji, simply surround the emoji name with two colons: :emoji-name: . A list of the available emojis can be found at emojipedia .","title":"Emojis"},{"location":"contributing/docs/markdown-cheatsheet/#content-tabs","text":"Content Tabs have been used throughout this page. === \"Tab Title A\" some markdown content === \"Tab Title B\" some other markdown content Tab Title A some markdown content Tab Title B some other markdown content","title":"Content Tabs"},{"location":"contributing/docs/markdown-cheatsheet/#footnotes","text":"Markdown Footnotes[^1] are supported. [ ^1 ]: some footnote information Rendered Footnotes 1 are supported. here's some footnote text \u21a9","title":"Footnotes"},{"location":"contributing/docs/markdown-lint/","text":"Markdownlint \u00b6 Markdownlint is used to ensure consistency in the structure of the Markdown files across the docs pages. Perform Linting \u00b6 To run the markdownlint linter run: just lint-markdown . Rules \u00b6 The Rules for markdownlint explain what's enforced with examples. Ignoring Violations \u00b6 In rare cases, it's necessary to ignore markdown lint violations . Configuration \u00b6 The .markdownlint-cli2.yaml configuration file is used for IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for markdownlint.","title":"Markdownlint"},{"location":"contributing/docs/markdown-lint/#markdownlint","text":"Markdownlint is used to ensure consistency in the structure of the Markdown files across the docs pages.","title":"Markdownlint"},{"location":"contributing/docs/markdown-lint/#perform-linting","text":"To run the markdownlint linter run: just lint-markdown .","title":"Perform Linting"},{"location":"contributing/docs/markdown-lint/#rules","text":"The Rules for markdownlint explain what's enforced with examples.","title":"Rules"},{"location":"contributing/docs/markdown-lint/#ignoring-violations","text":"In rare cases, it's necessary to ignore markdown lint violations .","title":"Ignoring Violations"},{"location":"contributing/docs/markdown-lint/#configuration","text":"The .markdownlint-cli2.yaml configuration file is used for IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for markdownlint.","title":"Configuration"},{"location":"contributing/docs/vale/","text":"Prose Linting with Vale \u00b6 These docs use Vale to ensure consistency of prose style. Perform Linting \u00b6 Run just lint-prose to specifically lint the documentation prose. Style Guide \u00b6 Vale uses the Microsoft Writing Style Guide . The styles for Vale can be found in docs/styles/Microsoft and were taken from here . Configuration \u00b6 The .vale.ini file at the root of the repository is used to configure Vale for both IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for Vale.","title":"Prose Linting with Vale"},{"location":"contributing/docs/vale/#prose-linting-with-vale","text":"These docs use Vale to ensure consistency of prose style.","title":"Prose Linting with Vale"},{"location":"contributing/docs/vale/#perform-linting","text":"Run just lint-prose to specifically lint the documentation prose.","title":"Perform Linting"},{"location":"contributing/docs/vale/#style-guide","text":"Vale uses the Microsoft Writing Style Guide . The styles for Vale can be found in docs/styles/Microsoft and were taken from here .","title":"Style Guide"},{"location":"contributing/docs/vale/#configuration","text":"The .vale.ini file at the root of the repository is used to configure Vale for both IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for Vale.","title":"Configuration"},{"location":"how-to/overview/","text":"Overview \u00b6 How-To Guides are goal oriented step-by-step instructions for specific problems. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! How-To Guide Description Upgrade to 2.0 An overview of the differences between JTE 1.X and 2.X","title":"Overview"},{"location":"how-to/overview/#overview","text":"How-To Guides are goal oriented step-by-step instructions for specific problems. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! How-To Guide Description Upgrade to 2.0 An overview of the differences between JTE 1.X and 2.X","title":"Overview"},{"location":"how-to/upgrade-2.0/","text":"2.0 Upgrade Guide \u00b6 This page is going to help walk you through the breaking changes associated with 2.0. Library File Structure \u00b6 To support Library Resources , the file structure of libraries has been reorganized. Pre-2.0 . \u251c\u2500\u2500 libraryA // libraries are directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config at root \u2502 \u2514\u2500\u2500 someOtherStep.groovy // step files at root of library directory \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u2514\u2500\u2500 someOtherStep.groovy Post-2.0 . \u251c\u2500\u2500 libraryA // libraries are still directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config still at root \u2502 \u251c\u2500\u2500 resources // new resources directory! \u2502 \u2502 \u2514\u2500\u2500 someScript.sh \u2502 \u2514\u2500\u2500 steps // steps are now in a steps directory \u2502 \u2514\u2500\u2500 someOtherStep.groovy \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 someData.json \u2514\u2500\u2500 steps \u2514\u2500\u2500 someOtherStep.groovy @override & @merge annotations \u00b6 Previously, the ability to govern pipeline configuration changes were confined to the block-level - with no way to govern individual fields. To address this, JTE has pivoted from the flags merge=true & override=true to the annotations @merge & @override . These annotations can be placed on individual fields within a block, enabling field-level governance. Pre-2.0 someBlock { merge = true // future configs can add fields to this block my_governed_field = \"some value\" // cannot be modified } anotherBlock { override = true // entire block can be overridden. no way to only override a field in a block. may_not_be_changed = true default_value_may_be_changed = true } Post-2.0 @merge someBlock { // future configs can add fields to this block my_governed_field = \"some value\" } anotherBlock { // future configs can't add fields to this block may_not_be_changed = true // not modifiable @override default_value = true // may be overridden } Top level pipeline configuration values and the jte{} block \u00b6 Previously, there were top level configuration values like allow_scm_jenkinsfile and pipeline_template . These values are now in the jte block in the pipeline_config Pre-2.0 allow_scm_jenkinsfile = false pipeline_template = \"my_template\" libraries {} // just here to show the relation to the root Post-2.0 jte { allow_scm_jenkinsfile = false pipeline_template = \"my_template\" } libraries {} // just here to show relation to the root Lifecycle Hook: hookContext \u00b6 JTE provides some syntactic sugar by means of autowiring variables to library steps to simplify library development. Previously, library steps that implemented lifecycle hooks were required to accept a method parameter to accept the hook context. This parameter was typically called context but could be called anything. Pre-2.0 @AfterStep ({ context . step == \"build\" }) // variable called context void call ( context ){ // hooks required to accept a method parameter println \"running after the ${context.step} step\" } Post-2.0 @AfterStep ({ hookContext . step == \"build\" }) // variable called hookContext void call (){ // no method parameter required println \"running after the ${hookContext.step} step\" // hookContext variable autowired } Configuration Changes \u00b6 JTE 2.0 resulted in significant refactoring of the codebase and underlying package structure. Global Configurations \u00b6 There have been updates to the underlying class structure of the Global Governance Tier configured in Manage Jenkins > Configure System > Jenkins Templating Engine . This will impact the Jenkins Configuration as Code (JCasC) YAML schema used to configure JTE. Tip It's recommended to configure the Global Governance Tier manually the way you require and exporting the JCasC YAML to see the schema required to automate configuring JTE. Job Configurations \u00b6 There have been updates to the underlying package and class structure for JTE as a whole as well as feature development for ad hoc pipeline jobs. This impacts Job DSL scripts used to configure jobs utilizing JTE. JTE also now supports fetching the Pipeline Configuration and Pipeline Template for a one-off pipeline job, which results in some changes to the structure of Job DSL for ad hoc pipeline jobs. Tip Job DSL supports Dynamic DSL which means that Job DSL supports the Jenkins Templating Engine settings. It's recommended to utilize the Job DSL API Viewer on your Jenkins Instance once JTE 2.0 has been installed to see how to configure JTE settings.","title":"2.0 Upgrade Guide"},{"location":"how-to/upgrade-2.0/#20-upgrade-guide","text":"This page is going to help walk you through the breaking changes associated with 2.0.","title":"2.0 Upgrade Guide"},{"location":"how-to/upgrade-2.0/#library-file-structure","text":"To support Library Resources , the file structure of libraries has been reorganized. Pre-2.0 . \u251c\u2500\u2500 libraryA // libraries are directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config at root \u2502 \u2514\u2500\u2500 someOtherStep.groovy // step files at root of library directory \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u2514\u2500\u2500 someOtherStep.groovy Post-2.0 . \u251c\u2500\u2500 libraryA // libraries are still directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config still at root \u2502 \u251c\u2500\u2500 resources // new resources directory! \u2502 \u2502 \u2514\u2500\u2500 someScript.sh \u2502 \u2514\u2500\u2500 steps // steps are now in a steps directory \u2502 \u2514\u2500\u2500 someOtherStep.groovy \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 someData.json \u2514\u2500\u2500 steps \u2514\u2500\u2500 someOtherStep.groovy","title":"Library File Structure"},{"location":"how-to/upgrade-2.0/#override-merge-annotations","text":"Previously, the ability to govern pipeline configuration changes were confined to the block-level - with no way to govern individual fields. To address this, JTE has pivoted from the flags merge=true & override=true to the annotations @merge & @override . These annotations can be placed on individual fields within a block, enabling field-level governance. Pre-2.0 someBlock { merge = true // future configs can add fields to this block my_governed_field = \"some value\" // cannot be modified } anotherBlock { override = true // entire block can be overridden. no way to only override a field in a block. may_not_be_changed = true default_value_may_be_changed = true } Post-2.0 @merge someBlock { // future configs can add fields to this block my_governed_field = \"some value\" } anotherBlock { // future configs can't add fields to this block may_not_be_changed = true // not modifiable @override default_value = true // may be overridden }","title":"@override &amp; @merge annotations"},{"location":"how-to/upgrade-2.0/#top-level-pipeline-configuration-values-and-the-jte-block","text":"Previously, there were top level configuration values like allow_scm_jenkinsfile and pipeline_template . These values are now in the jte block in the pipeline_config Pre-2.0 allow_scm_jenkinsfile = false pipeline_template = \"my_template\" libraries {} // just here to show the relation to the root Post-2.0 jte { allow_scm_jenkinsfile = false pipeline_template = \"my_template\" } libraries {} // just here to show relation to the root","title":"Top level pipeline configuration values and the jte{} block"},{"location":"how-to/upgrade-2.0/#lifecycle-hook-hookcontext","text":"JTE provides some syntactic sugar by means of autowiring variables to library steps to simplify library development. Previously, library steps that implemented lifecycle hooks were required to accept a method parameter to accept the hook context. This parameter was typically called context but could be called anything. Pre-2.0 @AfterStep ({ context . step == \"build\" }) // variable called context void call ( context ){ // hooks required to accept a method parameter println \"running after the ${context.step} step\" } Post-2.0 @AfterStep ({ hookContext . step == \"build\" }) // variable called hookContext void call (){ // no method parameter required println \"running after the ${hookContext.step} step\" // hookContext variable autowired }","title":"Lifecycle Hook: hookContext"},{"location":"how-to/upgrade-2.0/#configuration-changes","text":"JTE 2.0 resulted in significant refactoring of the codebase and underlying package structure.","title":"Configuration Changes"},{"location":"how-to/upgrade-2.0/#global-configurations","text":"There have been updates to the underlying class structure of the Global Governance Tier configured in Manage Jenkins > Configure System > Jenkins Templating Engine . This will impact the Jenkins Configuration as Code (JCasC) YAML schema used to configure JTE. Tip It's recommended to configure the Global Governance Tier manually the way you require and exporting the JCasC YAML to see the schema required to automate configuring JTE.","title":"Global Configurations"},{"location":"how-to/upgrade-2.0/#job-configurations","text":"There have been updates to the underlying package and class structure for JTE as a whole as well as feature development for ad hoc pipeline jobs. This impacts Job DSL scripts used to configure jobs utilizing JTE. JTE also now supports fetching the Pipeline Configuration and Pipeline Template for a one-off pipeline job, which results in some changes to the structure of Job DSL for ad hoc pipeline jobs. Tip Job DSL supports Dynamic DSL which means that Job DSL supports the Jenkins Templating Engine settings. It's recommended to utilize the Job DSL API Viewer on your Jenkins Instance once JTE 2.0 has been installed to see how to configure JTE settings.","title":"Job Configurations"},{"location":"reference/autowired-variables/","text":"Autowired Variables \u00b6 The JTE framework often makes use of autowired variables to share both configuration data and contextual information. This page outlines the various autowired variables, their scope, and what data they provide. Overview \u00b6 Variable Description Scope pipelineConfig Represents the aggregated Pipeline Configuration Accessible everywhere jte The Primitive Namespace object Accessible everywhere config Represents a library's configuration provided by the aggregated Pipeline Configuration Within Library Steps stepContext Enables step introspection. Especially helpful when using Step Aliasing Within Library Steps hookContext Represents contextual information for Lifecycle Hooks Within Library Steps Autowired Global Variables \u00b6 pipelineConfig \u00b6 The pipelineConfig is accessible from everywhere and allows access to the aggregated Pipeline Configuration as a Map . Example Usage of pipelineConfig An example of accessing the Pipeline Configuration via pipelineConfig : Pipeline Configuration keywords { foo = \"bar\" } random_field = 11 Pipeline Template println pipelineconfig . keywords . foo println pipelineConfig . random_field jte \u00b6 The jte variable represents the Primitive Namespace . All loaded Pipeline Primitives for a Run can be accessed via the jte variable This is different from the pipelineConfig variable. The pipelineConfig variable gives a Map representation of the aggregated Pipeline Configuration whereas the jte variable allows access to the actual Pipeline Primitive objects . Example Usage of jte Assume there's a gradle and an npm library that both contribute a build() step. By default, loading would result in the pipeline failing. However, you can perform Step Overloading by setting jte.permissive_initialization to True . The jte would be used in this scenario to invoke the build() step from the gradle and npm libraries. Pipeline Configuration jte { permissive_initialization = true } libraries { gradle npm } Pipeline Template // invoke the gradle build step jte . libraries . gradle . build () // invoke the npm build step jte . libraries . npm . build () jte block vs jte variable You may have noticed in the example above that a jte{} block is used in the Pipeline Configuration and a jte variable is used in the Pipeline Template. These are different things. The jte{} block refers to framework-level feature flags as explained on the Pipeline Configuration schema page. The jte variable refers to the Pipeline Primitive Namespace variable. steps \u00b6 The steps variable doesn't technically come from JTE. It's a feature of all Jenkins Pipelines. The steps variable allows direct access to invoke Jenkins Pipeline DSL Steps. This variable is most commonly used when invoking Jenkins Pipeline DSL Steps from a Library Class or when Overloading Steps . Autowired Library Step Variables \u00b6 The following variables are only accessible within Library Steps . config \u00b6 The config variable represents the library configuration for the library that contributed the step as a Map . Example Usage of config Assume there's a gradle library that contributes a build() step. Pipeline Configuration libraries { gradle { version = \"6.3\" } } build.groovy void call (){ String gradleVersion = config . version } hookContext \u00b6 The hookContext variable provides information about the current step to Lifecycle Hooks . Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. Example hookContext usage The following example shows how to use the hookContext variable so that a Lifecycle Hook only triggers after the build() step from the gradle library. Lifecycle Hook Step @AfterStep ({ hookContext . library == \"gradle\" && hookContext . step == \"build\" }) void call (){ println \"running after the ${hookContext.library}'s ${hookContext.step} step\" } stageContext \u00b6 The stageContext variable provides information about the current Stage . Property Type Description name String The name of the current Stage being executed. null if step isn't being executed as part of a Stage. args Map The named parameters provided to the Stage. An empty Map if no parameters provided. Example usage of stageContext The following example shows how to modify step behavior based upon Stage context. Pipeline Configuration libraries { npm // contributes unit_test() sonarqube // contributes static_code_analysis() } stages { ci { unit_test static_code_analysis } } NPM: unit_test.groovy void call (){ if ( stageContext . name == \"ci\" ){ println \"running as part of the ci Stage\" } } stepContext \u00b6 The stepContext allows step introspection, such as querying the name of the library providing the step or the current name of the step. Property Type Description library String The name of the library that contributed the step name String The current name of the step. May differ from the basename of the step's groovy file if using Step Aliasing isAlias Boolean Is true when stepContext.name refers to an alias Example usage of stepContext Aliased Step @StepAlias ([ \"build\" , \"unit_test\" ]) void call (){ println \"currently running as ${stepContext.name}\" } Pipeline Template build () // prints \"currently running as build\" unit_test () // prints \"currently running as unit_test\"","title":"Autowired Variables"},{"location":"reference/autowired-variables/#autowired-variables","text":"The JTE framework often makes use of autowired variables to share both configuration data and contextual information. This page outlines the various autowired variables, their scope, and what data they provide.","title":"Autowired Variables"},{"location":"reference/autowired-variables/#overview","text":"Variable Description Scope pipelineConfig Represents the aggregated Pipeline Configuration Accessible everywhere jte The Primitive Namespace object Accessible everywhere config Represents a library's configuration provided by the aggregated Pipeline Configuration Within Library Steps stepContext Enables step introspection. Especially helpful when using Step Aliasing Within Library Steps hookContext Represents contextual information for Lifecycle Hooks Within Library Steps","title":"Overview"},{"location":"reference/autowired-variables/#autowired-global-variables","text":"","title":"Autowired Global Variables"},{"location":"reference/autowired-variables/#pipelineconfig","text":"The pipelineConfig is accessible from everywhere and allows access to the aggregated Pipeline Configuration as a Map . Example Usage of pipelineConfig An example of accessing the Pipeline Configuration via pipelineConfig : Pipeline Configuration keywords { foo = \"bar\" } random_field = 11 Pipeline Template println pipelineconfig . keywords . foo println pipelineConfig . random_field","title":"pipelineConfig"},{"location":"reference/autowired-variables/#jte","text":"The jte variable represents the Primitive Namespace . All loaded Pipeline Primitives for a Run can be accessed via the jte variable This is different from the pipelineConfig variable. The pipelineConfig variable gives a Map representation of the aggregated Pipeline Configuration whereas the jte variable allows access to the actual Pipeline Primitive objects . Example Usage of jte Assume there's a gradle and an npm library that both contribute a build() step. By default, loading would result in the pipeline failing. However, you can perform Step Overloading by setting jte.permissive_initialization to True . The jte would be used in this scenario to invoke the build() step from the gradle and npm libraries. Pipeline Configuration jte { permissive_initialization = true } libraries { gradle npm } Pipeline Template // invoke the gradle build step jte . libraries . gradle . build () // invoke the npm build step jte . libraries . npm . build () jte block vs jte variable You may have noticed in the example above that a jte{} block is used in the Pipeline Configuration and a jte variable is used in the Pipeline Template. These are different things. The jte{} block refers to framework-level feature flags as explained on the Pipeline Configuration schema page. The jte variable refers to the Pipeline Primitive Namespace variable.","title":"jte"},{"location":"reference/autowired-variables/#steps","text":"The steps variable doesn't technically come from JTE. It's a feature of all Jenkins Pipelines. The steps variable allows direct access to invoke Jenkins Pipeline DSL Steps. This variable is most commonly used when invoking Jenkins Pipeline DSL Steps from a Library Class or when Overloading Steps .","title":"steps"},{"location":"reference/autowired-variables/#autowired-library-step-variables","text":"The following variables are only accessible within Library Steps .","title":"Autowired Library Step Variables"},{"location":"reference/autowired-variables/#config","text":"The config variable represents the library configuration for the library that contributed the step as a Map . Example Usage of config Assume there's a gradle library that contributes a build() step. Pipeline Configuration libraries { gradle { version = \"6.3\" } } build.groovy void call (){ String gradleVersion = config . version }","title":"config"},{"location":"reference/autowired-variables/#hookcontext","text":"The hookContext variable provides information about the current step to Lifecycle Hooks . Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. Example hookContext usage The following example shows how to use the hookContext variable so that a Lifecycle Hook only triggers after the build() step from the gradle library. Lifecycle Hook Step @AfterStep ({ hookContext . library == \"gradle\" && hookContext . step == \"build\" }) void call (){ println \"running after the ${hookContext.library}'s ${hookContext.step} step\" }","title":"hookContext"},{"location":"reference/autowired-variables/#stagecontext","text":"The stageContext variable provides information about the current Stage . Property Type Description name String The name of the current Stage being executed. null if step isn't being executed as part of a Stage. args Map The named parameters provided to the Stage. An empty Map if no parameters provided. Example usage of stageContext The following example shows how to modify step behavior based upon Stage context. Pipeline Configuration libraries { npm // contributes unit_test() sonarqube // contributes static_code_analysis() } stages { ci { unit_test static_code_analysis } } NPM: unit_test.groovy void call (){ if ( stageContext . name == \"ci\" ){ println \"running as part of the ci Stage\" } }","title":"stageContext"},{"location":"reference/autowired-variables/#stepcontext","text":"The stepContext allows step introspection, such as querying the name of the library providing the step or the current name of the step. Property Type Description library String The name of the library that contributed the step name String The current name of the step. May differ from the basename of the step's groovy file if using Step Aliasing isAlias Boolean Is true when stepContext.name refers to an alias Example usage of stepContext Aliased Step @StepAlias ([ \"build\" , \"unit_test\" ]) void call (){ println \"currently running as ${stepContext.name}\" } Pipeline Template build () // prints \"currently running as build\" unit_test () // prints \"currently running as unit_test\"","title":"stepContext"},{"location":"reference/governance-tier/","text":"Governance Tier \u00b6 Governance Tiers are nodes in the Configuration Hierarchy . This page explain the options to configure a Governance Tier. Overview \u00b6 Governance Tiers can store three important things: a Pipeline Catalog , a Pipeline Configuration , and a list of Library Sources . The configuration for Library Sources stands alone. The configuration for the Pipeline Catalog and Pipeline Configuration are grouped together. Library Sources \u00b6 Governance Tiers can configure a list of Library Sources . When adding a Library Source, there will be a dropdown to determine the type of Library Provider. A Library Provider is a retrieval mechanism for libraries. JTE packages two types of Library Providers as part of the plugin: \"From SCM\" and \"From Plugin.\" Note Users will only see the \"From Plugin\" option available in the dropdown if a plugin has been installed that's capable of providing libraries. The ordering of Library Sources in the list impacts Library Resolution . From Remote Repository \u00b6 When configuring a Library Source that fetches from a remote repository, users can configure the type of source code repository as well as the configuration base directory. The configuration base directory is the path within the remote repository where the libraries can be found. Each subdirectory within the configuration base directory will be treated as a library. Info Refer to the Library Structure for how to organize files within a library directory. Pipeline Catalog \u00b6 Default Pipeline Template \u00b6 Governance Tier Type Location of Default Pipeline Template From a Remote Repository a Jenkinsfile at the root of the configuration base directory From the Jenkins Console a dedicated text box labeled 'Default Template' Named Pipeline Templates \u00b6 Governance Tier Type Location of Named Pipeline Templates From a Remote Repository groovy files within a pipeline_templates directory located at the root of the configuration base directory From the Jenkins Console a list of named templates can be added directly in the Jenkins Console Pipeline Configuration \u00b6 Governance Tier Type Location of the Pipeline Configuration From a Remote Repository a pipeline_config.groovy at the root of the configuration base directory From the Jenkins Console a text field labeled Pipeline Configuration","title":"Governance Tier"},{"location":"reference/governance-tier/#governance-tier","text":"Governance Tiers are nodes in the Configuration Hierarchy . This page explain the options to configure a Governance Tier.","title":"Governance Tier"},{"location":"reference/governance-tier/#overview","text":"Governance Tiers can store three important things: a Pipeline Catalog , a Pipeline Configuration , and a list of Library Sources . The configuration for Library Sources stands alone. The configuration for the Pipeline Catalog and Pipeline Configuration are grouped together.","title":"Overview"},{"location":"reference/governance-tier/#library-sources","text":"Governance Tiers can configure a list of Library Sources . When adding a Library Source, there will be a dropdown to determine the type of Library Provider. A Library Provider is a retrieval mechanism for libraries. JTE packages two types of Library Providers as part of the plugin: \"From SCM\" and \"From Plugin.\" Note Users will only see the \"From Plugin\" option available in the dropdown if a plugin has been installed that's capable of providing libraries. The ordering of Library Sources in the list impacts Library Resolution .","title":"Library Sources"},{"location":"reference/governance-tier/#from-remote-repository","text":"When configuring a Library Source that fetches from a remote repository, users can configure the type of source code repository as well as the configuration base directory. The configuration base directory is the path within the remote repository where the libraries can be found. Each subdirectory within the configuration base directory will be treated as a library. Info Refer to the Library Structure for how to organize files within a library directory.","title":"From Remote Repository"},{"location":"reference/governance-tier/#pipeline-catalog","text":"","title":"Pipeline Catalog"},{"location":"reference/governance-tier/#default-pipeline-template","text":"Governance Tier Type Location of Default Pipeline Template From a Remote Repository a Jenkinsfile at the root of the configuration base directory From the Jenkins Console a dedicated text box labeled 'Default Template'","title":"Default Pipeline Template"},{"location":"reference/governance-tier/#named-pipeline-templates","text":"Governance Tier Type Location of Named Pipeline Templates From a Remote Repository groovy files within a pipeline_templates directory located at the root of the configuration base directory From the Jenkins Console a list of named templates can be added directly in the Jenkins Console","title":"Named Pipeline Templates"},{"location":"reference/governance-tier/#pipeline-configuration","text":"Governance Tier Type Location of the Pipeline Configuration From a Remote Repository a pipeline_config.groovy at the root of the configuration base directory From the Jenkins Console a text field labeled Pipeline Configuration","title":"Pipeline Configuration"},{"location":"reference/library-configuration-schema/","text":"Library Configuration Schema \u00b6 This page outlines the schema for Library Configuration Files . Library Configuration Validation \u00b6 Schema \u00b6 fields { [ 1 ] required {} [ 2 ] optional {} [ 3 ] } The fields{} block is used to declare what properties are expected in a library configuration. The required{} block is used to declare required fields. The optional{} block is used to declare optional fields. Within the required and optional blocks, list the parameters the library supports in a parameterName = <Validation Type> format. Note If a library doesn't include a library configuration file, then users can supply arbitrary parameters to the library from the Pipeline Configuration. If a library does include a library configuration file, then users will only be able to supply parameters that are listed within the required and optional blocks. The presence of extraneous parameters will fail the build. Supported Validations \u00b6 The library configuration supports several different validation types for library parameters. Type Validation \u00b6 Type validation confirms that a library parameter is an instance of a particular type. The current options for data types to test for are: boolean / Boolean String Integer / int Double BigDecimal Float Number Type Validation Example Library Configuration File fields { required { parameterA = String [ 1 ] parameterB = Number [ 2 ] parameterC = Boolean [ 3 ] } optional { parameterD = String [ 4 ] parameterE = Boolean [ 5 ] } } ensures that parameterA was configured and is an instance of a String ensures that parameterB was configured and is an instance of a Number ensures that parameterC was configured and is an instance of a Boolean if parameterD was configured, ensures it's a String if parameterE was configured, ensures it's a Boolean Enum Validation \u00b6 The enum validation ensures that a library parameter value is one of the options defined by a list in the library configuration. Enum Validation Example Library Configuration File fields { required { parameterA = [ \"a\" , \"b\" , 11 ] [ 1 ] } } ensures that parameterA was configured and is set to either 'a', 'b', or 11 Regular Expression Validation \u00b6 Regular expression validation uses Groovy's match operator to determine if the parameter value is matched by the regular expression. Regular Expression Example Library Configuration File fields { required { parameterA = ~ /^s.*/ [ 1 ] } } ensures that parameterA starts with s Nested Parameters \u00b6 Library parameters can be arbitrarily nested within the pipeline configuration. For example, the following pipeline configuration would be valid to pass the example.nestedParameter parameter to a library named testing . Pipeline Configuration libraries { testing { example { nestedParameter = 11 } } } Library Configuration fields { required { example { nestedParameter = Number } } } Tip To validate nested library parameters in the library configuration, nest their validation in the same structure within the required or optional blocks.","title":"Library Configuration Schema"},{"location":"reference/library-configuration-schema/#library-configuration-schema","text":"This page outlines the schema for Library Configuration Files .","title":"Library Configuration Schema"},{"location":"reference/library-configuration-schema/#library-configuration-validation","text":"","title":"Library Configuration Validation"},{"location":"reference/library-configuration-schema/#schema","text":"fields { [ 1 ] required {} [ 2 ] optional {} [ 3 ] } The fields{} block is used to declare what properties are expected in a library configuration. The required{} block is used to declare required fields. The optional{} block is used to declare optional fields. Within the required and optional blocks, list the parameters the library supports in a parameterName = <Validation Type> format. Note If a library doesn't include a library configuration file, then users can supply arbitrary parameters to the library from the Pipeline Configuration. If a library does include a library configuration file, then users will only be able to supply parameters that are listed within the required and optional blocks. The presence of extraneous parameters will fail the build.","title":"Schema"},{"location":"reference/library-configuration-schema/#supported-validations","text":"The library configuration supports several different validation types for library parameters.","title":"Supported Validations"},{"location":"reference/library-configuration-schema/#type-validation","text":"Type validation confirms that a library parameter is an instance of a particular type. The current options for data types to test for are: boolean / Boolean String Integer / int Double BigDecimal Float Number Type Validation Example Library Configuration File fields { required { parameterA = String [ 1 ] parameterB = Number [ 2 ] parameterC = Boolean [ 3 ] } optional { parameterD = String [ 4 ] parameterE = Boolean [ 5 ] } } ensures that parameterA was configured and is an instance of a String ensures that parameterB was configured and is an instance of a Number ensures that parameterC was configured and is an instance of a Boolean if parameterD was configured, ensures it's a String if parameterE was configured, ensures it's a Boolean","title":"Type Validation"},{"location":"reference/library-configuration-schema/#enum-validation","text":"The enum validation ensures that a library parameter value is one of the options defined by a list in the library configuration. Enum Validation Example Library Configuration File fields { required { parameterA = [ \"a\" , \"b\" , 11 ] [ 1 ] } } ensures that parameterA was configured and is set to either 'a', 'b', or 11","title":"Enum Validation"},{"location":"reference/library-configuration-schema/#regular-expression-validation","text":"Regular expression validation uses Groovy's match operator to determine if the parameter value is matched by the regular expression. Regular Expression Example Library Configuration File fields { required { parameterA = ~ /^s.*/ [ 1 ] } } ensures that parameterA starts with s","title":"Regular Expression Validation"},{"location":"reference/library-configuration-schema/#nested-parameters","text":"Library parameters can be arbitrarily nested within the pipeline configuration. For example, the following pipeline configuration would be valid to pass the example.nestedParameter parameter to a library named testing . Pipeline Configuration libraries { testing { example { nestedParameter = 11 } } } Library Configuration fields { required { example { nestedParameter = Number } } } Tip To validate nested library parameters in the library configuration, nest their validation in the same structure within the required or optional blocks.","title":"Nested Parameters"},{"location":"reference/overview/","text":"Overview \u00b6 Reference pages are information oriented descriptions of JTE mechanics. Learn More \u00b6 Page Description Pipeline Configuration Schema Learn what can be configured in the Pipeline Configuration Autowired Variables Learn what variables JTE injects into various scopes to share context and data Library Configuration Schema Learn how to create a Library Configuration File","title":"Overview"},{"location":"reference/overview/#overview","text":"Reference pages are information oriented descriptions of JTE mechanics.","title":"Overview"},{"location":"reference/overview/#learn-more","text":"Page Description Pipeline Configuration Schema Learn what can be configured in the Pipeline Configuration Autowired Variables Learn what variables JTE injects into various scopes to share context and data Library Configuration Schema Learn how to create a Library Configuration File","title":"Learn More"},{"location":"reference/pipeline-configuration-schema/","text":"Pipeline Configuration Schema \u00b6 Overview \u00b6 Check out the Pipeline Configuration page for an explanation of the Pipeline Configuration's purpose and syntax. Root-Level Blocks \u00b6 jte \u00b6 The jte{} block of the Pipeline Configuration is reserved for fields that change framework behavior. key description type default allow_scm_jenkinsfile Determines whether a Jenkinsfile in a source code repository will be used when determining the Pipeline Template. Refer to Pipeline Template Selection for more information. Boolean True permissive_initialization Determine whether to fail the build during pipeline initialization if multiple Pipeline Primitives with conflicting names are loaded. Setting to True will allow multiple Pipeline Primitives with the same name to be loaded. Boolean False pipeline_template Specify a named template from the Pipeline Catalog to use. String null reverse_library_resolution Determine whether to reverse the order that Library Sources are queried for a library. Refer to Library Resolution for more information. Boolean False Example JTE Block jte { allow_scm_jenkinsfile = False permissive_initialization = True pipeline_template = \"my-named-template.groovy\" reverse_library_resolution = True } template_methods \u00b6 The template_methods{} block is used to define the names of steps to create a no-op placeholder for if a library doesn't provide the step's implementation. Refer to Placeholder Steps for more information. Example template_methods block template_methods { unit_test build deploy_to } libraries \u00b6 The libraries{} block determines which libraries to load. The block names within libraries must reference a library within a configured Library Source available to the job. Refer to the Library Development Overview for more information. Example libraries block libraries { library_A { param1 = \"foo\" param2 = \"bar\" ... } ... } stages \u00b6 The stages{} block is used to define Stages . Example stages block stages { stage_name { step1 step2 ... } ... } application_environments \u00b6 The application_environments{} block is used to define Application Environments . Example application_environments block application_environments { dev { long_name = \"Development\" } test prod } keywords \u00b6 The keywords{} block is used to define Keywords . Example keywords block keywords { main = ~ /^[Mm](ain|aster)$/ globals { foo = \"bar\" } }","title":"Pipeline Configuration Schema"},{"location":"reference/pipeline-configuration-schema/#pipeline-configuration-schema","text":"","title":"Pipeline Configuration Schema"},{"location":"reference/pipeline-configuration-schema/#overview","text":"Check out the Pipeline Configuration page for an explanation of the Pipeline Configuration's purpose and syntax.","title":"Overview"},{"location":"reference/pipeline-configuration-schema/#root-level-blocks","text":"","title":"Root-Level Blocks"},{"location":"reference/pipeline-configuration-schema/#jte","text":"The jte{} block of the Pipeline Configuration is reserved for fields that change framework behavior. key description type default allow_scm_jenkinsfile Determines whether a Jenkinsfile in a source code repository will be used when determining the Pipeline Template. Refer to Pipeline Template Selection for more information. Boolean True permissive_initialization Determine whether to fail the build during pipeline initialization if multiple Pipeline Primitives with conflicting names are loaded. Setting to True will allow multiple Pipeline Primitives with the same name to be loaded. Boolean False pipeline_template Specify a named template from the Pipeline Catalog to use. String null reverse_library_resolution Determine whether to reverse the order that Library Sources are queried for a library. Refer to Library Resolution for more information. Boolean False Example JTE Block jte { allow_scm_jenkinsfile = False permissive_initialization = True pipeline_template = \"my-named-template.groovy\" reverse_library_resolution = True }","title":"jte"},{"location":"reference/pipeline-configuration-schema/#template_methods","text":"The template_methods{} block is used to define the names of steps to create a no-op placeholder for if a library doesn't provide the step's implementation. Refer to Placeholder Steps for more information. Example template_methods block template_methods { unit_test build deploy_to }","title":"template_methods"},{"location":"reference/pipeline-configuration-schema/#libraries","text":"The libraries{} block determines which libraries to load. The block names within libraries must reference a library within a configured Library Source available to the job. Refer to the Library Development Overview for more information. Example libraries block libraries { library_A { param1 = \"foo\" param2 = \"bar\" ... } ... }","title":"libraries"},{"location":"reference/pipeline-configuration-schema/#stages","text":"The stages{} block is used to define Stages . Example stages block stages { stage_name { step1 step2 ... } ... }","title":"stages"},{"location":"reference/pipeline-configuration-schema/#application_environments","text":"The application_environments{} block is used to define Application Environments . Example application_environments block application_environments { dev { long_name = \"Development\" } test prod }","title":"application_environments"},{"location":"reference/pipeline-configuration-schema/#keywords","text":"The keywords{} block is used to define Keywords . Example keywords block keywords { main = ~ /^[Mm](ain|aster)$/ globals { foo = \"bar\" } }","title":"keywords"},{"location":"tutorials/overview/","text":"Overview \u00b6 Tutorials are learning oriented lessons to teach users about JTE. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over!","title":"Overview"},{"location":"tutorials/overview/#overview","text":"Tutorials are learning oriented lessons to teach users about JTE. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over!","title":"Overview"}]}